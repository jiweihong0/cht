#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆ Embedding ç³»çµ±
é å…ˆè¨ˆç®—ä¸¦å„²å­˜æ‰€æœ‰ RA è³‡æ–™çš„å‘é‡è¡¨ç¤ºï¼Œæå‡ç›¸ä¼¼åº¦æ¯”å°çš„æº–ç¢ºæ€§å’Œæ•ˆèƒ½
"""

import pandas as pd
import jieba
import numpy as np
import pickle
import os
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import re
import warnings
warnings.filterwarnings('ignore')

class EnhancedEmbeddingSystem:
    def __init__(self, csv_path='RA_data.csv', model_name='distiluse-base-multilingual-cased'):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆ Embedding ç³»çµ±
        Args:
            csv_path: CSV è³‡æ–™æª”æ¡ˆè·¯å¾‘
            model_name: Sentence-BERT æ¨¡å‹åç¨±
        """
        self.csv_path = csv_path
        self.model_name = model_name
        self.embeddings_file = 'ra_embeddings.pkl'
        self.metadata_file = 'ra_metadata.pkl'
        
        # å‘é‡åŒ–æ–¹æ³•
        self.use_sentence_transformer = True
        self.sentence_model = None
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=2000, 
            ngram_range=(1, 3),  # å¢åŠ åˆ°ä¸‰å…ƒçµ„
            min_df=1,
            max_df=0.95
        )
        
        # è³‡æ–™å­˜å„²
        self.df = None
        self.processed_texts = []
        self.categories = []
        self.asset_names = []
        self.embeddings = None
        self.tfidf_matrix = None
        
    def initialize_models(self):
        """åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹"""
        print("æ­£åœ¨åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹...")
        
        # å˜—è©¦è¼‰å…¥ Sentence-BERT æ¨¡å‹
        try:
            self.sentence_model = SentenceTransformer(self.model_name)
            print(f"âœ… æˆåŠŸè¼‰å…¥ Sentence-BERT æ¨¡å‹: {self.model_name}")
            self.use_sentence_transformer = True
        except Exception as e:
            print(f"âš ï¸  ç„¡æ³•è¼‰å…¥ Sentence-BERT æ¨¡å‹: {e}")
            print("ğŸ”„ æ”¹ç”¨ TF-IDF å‘é‡åŒ–")
            self.use_sentence_transformer = False
    
    def preprocess_text(self, text):
        """
        å¢å¼·ç‰ˆæ–‡æœ¬é è™•ç†
        Args:
            text: è¼¸å…¥æ–‡æœ¬
        Returns:
            è™•ç†å¾Œçš„æ–‡æœ¬
        """
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’Œæ•¸å­—
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\-\(\)]', ' ', text)
        
        # æ¨™æº–åŒ–æ‹¬è™Ÿå…§å®¹
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ä½¿ç”¨ jieba åˆ†è©
        words = list(jieba.cut(text))
        
        # ä¿ç•™æœ‰æ„ç¾©çš„è©å½™
        meaningful_words = []
        for word in words:
            word = word.strip()
            # ä¿ç•™é•·åº¦å¤§æ–¼1çš„è©ï¼Œæˆ–è€…æ˜¯è‹±æ–‡/æ•¸å­—
            if len(word) > 1 or re.match(r'[a-zA-Z0-9]', word):
                meaningful_words.append(word)
        
        processed = ' '.join(meaningful_words)
        return processed if processed else text  # å¦‚æœè™•ç†å¾Œç‚ºç©ºï¼Œè¿”å›åŸæ–‡
    
    def load_and_process_data(self):
        """è¼‰å…¥ä¸¦é è™•ç†è³‡æ–™"""
        print(f"æ­£åœ¨è¼‰å…¥è³‡æ–™: {self.csv_path}")
        
        try:
            self.df = pd.read_csv(self.csv_path, encoding='utf-8')
            print(f"âœ… è¼‰å…¥äº† {len(self.df)} ç­†è³‡æ–™")
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—: {e}")
            return False
        
        # é è™•ç†æ‰€æœ‰è³‡ç”¢åç¨±
        print("æ­£åœ¨é è™•ç†æ–‡æœ¬...")
        for _, row in self.df.iterrows():
            category = row['è³‡ç”¢é¡åˆ¥']
            asset_name = row['è³‡ç”¢åç¨±']
            
            # é è™•ç†è³‡ç”¢åç¨±
            processed_name = self.preprocess_text(asset_name)
            
            self.processed_texts.append(processed_name)
            self.categories.append(category)
            self.asset_names.append(asset_name)
        
        print(f"âœ… é è™•ç†å®Œæˆï¼Œå…± {len(self.processed_texts)} é …è³‡ç”¢")
        return True
    
    def compute_embeddings(self):
        """è¨ˆç®—æ‰€æœ‰è³‡ç”¢çš„ embedding å‘é‡"""
        if not self.processed_texts:
            print("âŒ æ²’æœ‰å¯è™•ç†çš„æ–‡æœ¬è³‡æ–™")
            return False
        
        print("æ­£åœ¨è¨ˆç®— embeddings...")
        
        if self.use_sentence_transformer and self.sentence_model:
            # ä½¿ç”¨ Sentence-BERT
            print("ğŸ¤– ä½¿ç”¨ Sentence-BERT è¨ˆç®—èªç¾©å‘é‡...")
            try:
                self.embeddings = self.sentence_model.encode(
                    self.processed_texts, 
                    show_progress_bar=True,
                    convert_to_numpy=True
                )
                print(f"âœ… Sentence-BERT embeddings å®Œæˆï¼Œç¶­åº¦: {self.embeddings.shape}")
            except Exception as e:
                print(f"âŒ Sentence-BERT è¨ˆç®—å¤±æ•—: {e}")
                self.use_sentence_transformer = False
        
        # åŒæ™‚è¨ˆç®— TF-IDF ä½œç‚ºå‚™ç”¨
        print("ğŸ“Š è¨ˆç®— TF-IDF å‘é‡...")
        try:
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.processed_texts)
            print(f"âœ… TF-IDF å‘é‡è¨ˆç®—å®Œæˆï¼Œç¶­åº¦: {self.tfidf_matrix.shape}")
        except Exception as e:
            print(f"âŒ TF-IDF è¨ˆç®—å¤±æ•—: {e}")
            return False
        
        return True
    
    def save_embeddings(self):
        """å„²å­˜ embeddings åˆ°æª”æ¡ˆ"""
        print("æ­£åœ¨å„²å­˜ embeddings...")
        
        # æº–å‚™è¦å„²å­˜çš„è³‡æ–™
        embedding_data = {
            'embeddings': self.embeddings,
            'tfidf_matrix': self.tfidf_matrix,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'processed_texts': self.processed_texts,
            'categories': self.categories,
            'asset_names': self.asset_names,
            'use_sentence_transformer': self.use_sentence_transformer,
            'model_name': self.model_name,
            'created_time': datetime.now().isoformat(),
            'data_hash': hash(str(self.processed_texts))  # ç”¨æ–¼æª¢æŸ¥è³‡æ–™æ˜¯å¦è®Šæ›´
        }
        
        try:
            with open(self.embeddings_file, 'wb') as f:
                pickle.dump(embedding_data, f)
            print(f"âœ… Embeddings å·²å„²å­˜åˆ°: {self.embeddings_file}")
            
            # å„²å­˜å¾Œè¨­è³‡æ–™
            metadata = {
                'total_assets': len(self.asset_names),
                'categories': list(set(self.categories)),
                'embedding_dimension': self.embeddings.shape[1] if self.embeddings is not None else None,
                'tfidf_features': self.tfidf_matrix.shape[1] if self.tfidf_matrix is not None else None,
                'created_time': datetime.now().isoformat(),
                'model_name': self.model_name
            }
            
            with open(self.metadata_file, 'wb') as f:
                pickle.dump(metadata, f)
            print(f"âœ… å¾Œè¨­è³‡æ–™å·²å„²å­˜åˆ°: {self.metadata_file}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å„²å­˜å¤±æ•—: {e}")
            return False
    
    def load_embeddings(self):
        """è¼‰å…¥é å…ˆè¨ˆç®—çš„ embeddings"""
        if not os.path.exists(self.embeddings_file):
            print(f"âš ï¸  æ‰¾ä¸åˆ° embeddings æª”æ¡ˆ: {self.embeddings_file}")
            return False
        
        try:
            print("æ­£åœ¨è¼‰å…¥é å…ˆè¨ˆç®—çš„ embeddings...")
            with open(self.embeddings_file, 'rb') as f:
                data = pickle.load(f)
            
            self.embeddings = data.get('embeddings')
            self.tfidf_matrix = data.get('tfidf_matrix')
            self.tfidf_vectorizer = data.get('tfidf_vectorizer')
            self.processed_texts = data.get('processed_texts', [])
            self.categories = data.get('categories', [])
            self.asset_names = data.get('asset_names', [])
            self.use_sentence_transformer = data.get('use_sentence_transformer', False)
            
            # æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§
            if not self.processed_texts or not self.categories:
                print("âŒ è¼‰å…¥çš„è³‡æ–™ä¸å®Œæ•´")
                return False
            
            print(f"âœ… æˆåŠŸè¼‰å…¥ {len(self.processed_texts)} é …è³‡ç”¢çš„ embeddings")
            print(f"ğŸ“… å»ºç«‹æ™‚é–“: {data.get('created_time', 'Unknown')}")
            
            # å¦‚æœä½¿ç”¨ Sentence-BERTï¼Œé‡æ–°è¼‰å…¥æ¨¡å‹
            if self.use_sentence_transformer:
                try:
                    self.sentence_model = SentenceTransformer(self.model_name)
                    print(f"âœ… é‡æ–°è¼‰å…¥ Sentence-BERT æ¨¡å‹: {self.model_name}")
                except Exception as e:
                    print(f"âš ï¸  ç„¡æ³•è¼‰å…¥ Sentence-BERT æ¨¡å‹ï¼Œæ”¹ç”¨ TF-IDF: {e}")
                    self.use_sentence_transformer = False
            
            return True
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥ embeddings å¤±æ•—: {e}")
            return False
    
    def compute_similarity(self, input_text, top_k=50):
        """
        è¨ˆç®—è¼¸å…¥æ–‡æœ¬èˆ‡æ‰€æœ‰è³‡ç”¢çš„ç›¸ä¼¼åº¦
        Args:
            input_text: è¼¸å…¥æ–‡æœ¬
            top_k: è¿”å›å‰ k å€‹æœ€ç›¸ä¼¼çš„çµæœ
        Returns:
            ç›¸ä¼¼åº¦çµæœåˆ—è¡¨
        """
        if not self.processed_texts:
            print("âŒ æ²’æœ‰è¼‰å…¥çš„ embeddings è³‡æ–™")
            return [], ""
        
        # é è™•ç†è¼¸å…¥æ–‡æœ¬
        processed_input = self.preprocess_text(input_text)
        print(f"è¼¸å…¥æ–‡æœ¬: {input_text}")
        print(f"è™•ç†å¾Œæ–‡æœ¬: {processed_input}")
        
        similarities_bert = None
        similarities_tfidf = None
        
        # ä½¿ç”¨ Sentence-BERT è¨ˆç®—ç›¸ä¼¼åº¦
        if self.use_sentence_transformer and self.sentence_model and self.embeddings is not None:
            try:
                print("ğŸ¤– ä½¿ç”¨ Sentence-BERT è¨ˆç®—ç›¸ä¼¼åº¦...")
                input_embedding = self.sentence_model.encode([processed_input])
                similarities_bert = cosine_similarity(input_embedding, self.embeddings)[0]
                print(f"âœ… Sentence-BERT ç›¸ä¼¼åº¦è¨ˆç®—å®Œæˆ")
            except Exception as e:
                print(f"âŒ Sentence-BERT ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
        
        # ä½¿ç”¨ TF-IDF è¨ˆç®—ç›¸ä¼¼åº¦
        if self.tfidf_matrix is not None and self.tfidf_vectorizer is not None:
            try:
                print("ğŸ“Š ä½¿ç”¨ TF-IDF è¨ˆç®—ç›¸ä¼¼åº¦...")
                input_tfidf = self.tfidf_vectorizer.transform([processed_input])
                similarities_tfidf = cosine_similarity(input_tfidf, self.tfidf_matrix)[0]
                print(f"âœ… TF-IDF ç›¸ä¼¼åº¦è¨ˆç®—å®Œæˆ")
            except Exception as e:
                print(f"âŒ TF-IDF ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
        
        # æ±ºå®šä½¿ç”¨å“ªç¨®ç›¸ä¼¼åº¦
        if similarities_bert is not None and similarities_tfidf is not None:
            # çµåˆå…©ç¨®æ–¹æ³• (åŠ æ¬Šå¹³å‡)
            similarities = 0.7 * similarities_bert + 0.3 * similarities_tfidf
            method_used = "Sentence-BERT + TF-IDF çµ„åˆ"
        elif similarities_bert is not None:
            similarities = similarities_bert
            method_used = "Sentence-BERT"
        elif similarities_tfidf is not None:
            similarities = similarities_tfidf
            method_used = "TF-IDF"
        else:
            print("âŒ æ‰€æœ‰ç›¸ä¼¼åº¦è¨ˆç®—æ–¹æ³•éƒ½å¤±æ•—")
            return [], processed_input
        
        print(f"âœ… ä½¿ç”¨æ–¹æ³•: {method_used}")
        
        # å»ºç«‹çµæœåˆ—è¡¨
        results = []
        for i, sim in enumerate(similarities):
            if i < len(self.categories) and i < len(self.asset_names):
                results.append({
                    'similarity': float(sim),
                    'category': self.categories[i],
                    'asset_name': self.asset_names[i],
                    'processed_text': self.processed_texts[i],
                    'index': i
                })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºä¸¦å–å‰ k å€‹
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k], processed_input
    
    def build_embeddings(self, force_rebuild=False):
        """
        å»ºç«‹ embeddingsï¼ˆä¸»è¦å…¥å£é»ï¼‰
        Args:
            force_rebuild: æ˜¯å¦å¼·åˆ¶é‡å»º
        """
        print("="*80)
        print("ğŸš€ å¢å¼·ç‰ˆ Embedding ç³»çµ±")
        print("="*80)
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        if not force_rebuild and self.load_embeddings():
            print("âœ… ä½¿ç”¨ç¾æœ‰çš„ embeddings")
            return True
        
        print("ğŸ”„ éœ€è¦é‡å»º embeddings...")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.initialize_models()
        
        # è¼‰å…¥å’Œé è™•ç†è³‡æ–™
        if not self.load_and_process_data():
            return False
        
        # è¨ˆç®— embeddings
        if not self.compute_embeddings():
            return False
        
        # å„²å­˜ embeddings
        if not self.save_embeddings():
            return False
        
        print("ğŸ‰ Embeddings å»ºç«‹å®Œæˆï¼")
        return True
    
    def get_statistics(self):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        if not self.categories:
            return {}
        
        category_count = {}
        for cat in self.categories:
            category_count[cat] = category_count.get(cat, 0) + 1
        
        return {
            'total_assets': len(self.asset_names),
            'categories': category_count,
            'embedding_method': "Sentence-BERT + TF-IDF" if self.use_sentence_transformer else "TF-IDF",
            'embedding_dimension': self.embeddings.shape[1] if self.embeddings is not None else None,
            'tfidf_features': self.tfidf_matrix.shape[1] if self.tfidf_matrix is not None else None
        }

def main():
    """ä¸»ç¨‹å¼ï¼šå»ºç«‹å’Œæ¸¬è©¦ embeddings"""
    # å»ºç«‹ embedding ç³»çµ±
    embedding_system = EnhancedEmbeddingSystem()
    
    # å»ºç«‹ embeddings
    if not embedding_system.build_embeddings():
        print("âŒ Embeddings å»ºç«‹å¤±æ•—")
        return
    
    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    stats = embedding_system.get_statistics()
    print("\nğŸ“Š ç³»çµ±çµ±è¨ˆè³‡è¨Š:")
    print(f"ç¸½è³‡ç”¢æ•¸é‡: {stats.get('total_assets', 0)}")
    print(f"Embedding æ–¹æ³•: {stats.get('embedding_method', 'Unknown')}")
    if stats.get('embedding_dimension'):
        print(f"èªç¾©å‘é‡ç¶­åº¦: {stats.get('embedding_dimension')}")
    if stats.get('tfidf_features'):
        print(f"TF-IDF ç‰¹å¾µæ•¸: {stats.get('tfidf_features')}")
    
    print("\nå„é¡åˆ¥è³‡ç”¢çµ±è¨ˆ:")
    for category, count in stats.get('categories', {}).items():
        print(f"  {category}: {count} é …")
    
    # æ¸¬è©¦ç›¸ä¼¼åº¦è¨ˆç®—
    print("\n" + "="*80)
    print("ğŸ§ª æ¸¬è©¦ç›¸ä¼¼åº¦è¨ˆç®—")
    print("="*80)
    
    test_cases = [
        "MySQL è³‡æ–™åº«ç®¡ç†ç³»çµ±",
        "å‚™ä»½æª”æ¡ˆå’Œæ—¥èªŒè¨˜éŒ„",
        "ç³»çµ±ç®¡ç†å“¡æ¬Šé™",
        "Windows ä½œæ¥­ç³»çµ±"
    ]
    
    for test_text in test_cases:
        print(f"\nğŸ” æ¸¬è©¦: {test_text}")
        results, processed = embedding_system.compute_similarity(test_text, top_k=5)
        
        if results:
            print(f"å‰ 5 å€‹æœ€ç›¸ä¼¼çš„é …ç›®:")
            for i, result in enumerate(results, 1):
                print(f"{i}. ã€{result['category']}ã€‘{result['asset_name']} "
                      f"(ç›¸ä¼¼åº¦: {result['similarity']:.4f})")
        else:
            print("âŒ æ²’æ‰¾åˆ°ç›¸ä¼¼é …ç›®")

if __name__ == "__main__":
    main()