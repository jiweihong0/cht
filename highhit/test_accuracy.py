#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è³‡ç”¢åˆ†é¡ç³»çµ±çœŸå¯¦å‘½ä¸­ç‡æ¸¬è©¦è…³æœ¬
è©•ä¼°åˆ†é¡å™¨çš„æº–ç¢ºæ€§ã€å¬å›ç‡ã€F1åˆ†æ•¸ç­‰æ€§èƒ½æŒ‡æ¨™
"""

import pandas as pd
import numpy as np
from text_classifier import TextClassifier
from similarity_analysis import SimilarityAnalyzer
from enhanced_demo_with_topk import enhanced_classify_with_threats
import random
import json
from datetime import datetime
from collections import defaultdict, Counter

class AccuracyTester:
    """æº–ç¢ºæ€§æ¸¬è©¦å™¨"""
    
    def __init__(self, ra_data_path='RA_data.csv'):
        """
        åˆå§‹åŒ–æ¸¬è©¦å™¨
        Args:
            ra_data_path: RAæ•¸æ“šæ–‡ä»¶è·¯å¾‘
        """
        self.ra_data_path = ra_data_path
        self.classifier = TextClassifier(ra_data_path)
        self.analyzer = SimilarityAnalyzer(ra_data_path)
        self.test_data = None
        self.results = []
        self.load_test_data()
    
    def load_test_data(self):
        """è¼‰å…¥æ¸¬è©¦æ•¸æ“š"""
        try:
            self.test_data = pd.read_csv(self.ra_data_path, encoding='utf-8')
            print(f"âœ… æˆåŠŸè¼‰å…¥æ¸¬è©¦æ•¸æ“šï¼š{len(self.test_data)} ç­†è¨˜éŒ„")
            print(f"ğŸ“Š åŒ…å« {len(self.test_data['è³‡ç”¢é¡åˆ¥'].unique())} å€‹ä¸åŒé¡åˆ¥")
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ¸¬è©¦æ•¸æ“šå¤±æ•—ï¼š{e}")
            self.test_data = pd.DataFrame()
    
    def create_test_variations(self, asset_name, num_variations=3):
        """
        ç‚ºæ¯å€‹è³‡ç”¢åç¨±å‰µå»ºè®ŠåŒ–ç‰ˆæœ¬ç”¨æ–¼æ¸¬è©¦
        Args:
            asset_name: åŸå§‹è³‡ç”¢åç¨±
            num_variations: éœ€è¦å‰µå»ºçš„è®ŠåŒ–æ•¸é‡
        Returns:
            list: åŒ…å«åŸå§‹åç¨±å’Œè®ŠåŒ–ç‰ˆæœ¬çš„åˆ—è¡¨
        """
        variations = [asset_name]  # åŒ…å«åŸå§‹åç¨±
        
        # ç§»é™¤æ‹¬è™Ÿå…§å®¹çš„ç‰ˆæœ¬
        if '(' in asset_name and ')' in asset_name:
            no_parentheses = asset_name.split('(')[0].strip()
            if no_parentheses and no_parentheses != asset_name:
                variations.append(no_parentheses)
        
        # åªä¿ç•™æ‹¬è™Ÿå…§å®¹çš„ç‰ˆæœ¬
        if '(' in asset_name and ')' in asset_name:
            parentheses_content = asset_name[asset_name.find('(')+1:asset_name.find(')')].strip()
            if parentheses_content:
                variations.append(parentheses_content)
        
        # æ·»åŠ ç©ºæ ¼è®ŠåŒ–
        if ' ' in asset_name:
            no_spaces = asset_name.replace(' ', '')
            variations.append(no_spaces)
        
        # å¤§å°å¯«è®ŠåŒ–
        variations.append(asset_name.lower())
        variations.append(asset_name.upper())
        
        # ç§»é™¤é‡è¤‡ä¸¦é™åˆ¶æ•¸é‡
        unique_variations = list(dict.fromkeys(variations))  # ä¿æŒé †åºå»é‡
        return unique_variations[:num_variations + 1]  # +1 å› ç‚ºåŒ…å«åŸå§‹åç¨±
    
    def test_single_classification(self, test_text, true_category, method='average'):
        """
        æ¸¬è©¦å–®å€‹åˆ†é¡çµæœ
        Args:
            test_text: æ¸¬è©¦æ–‡æœ¬
            true_category: çœŸå¯¦é¡åˆ¥
            method: åˆ†é¡æ–¹æ³•
        Returns:
            dict: æ¸¬è©¦çµæœ
        """
        try:
            # ä½¿ç”¨åˆ†é¡å™¨é€²è¡Œåˆ†é¡
            classification_result = self.classifier.classify_text(test_text, method=method)
            predicted_category = classification_result['best_prediction']
            
            # ä½¿ç”¨ç›¸ä¼¼åº¦åˆ†æ
            similarity_results, processed_text = self.analyzer.analyze_similarity(test_text)
            
            # ç¢ºå®šæœ€çµ‚é æ¸¬é¡åˆ¥
            final_category = predicted_category
            similarity_score = 0.0
            most_similar_asset = None
            
            if similarity_results:
                most_similar_category = similarity_results[0]['category']
                most_similar_asset = similarity_results[0]['asset_name']
                similarity_score = similarity_results[0]['similarity']
                
                # å¦‚æœç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œä½¿ç”¨ç›¸ä¼¼åº¦çµæœ
                if similarity_score > 0.7:
                    final_category = most_similar_category
            
            # åˆ¤æ–·æ˜¯å¦æ­£ç¢º
            is_correct = (final_category == true_category)
            
            result = {
                'test_text': test_text,
                'processed_text': processed_text,
                'true_category': true_category,
                'predicted_category': predicted_category,
                'final_category': final_category,
                'most_similar_asset': most_similar_asset,
                'similarity_score': similarity_score,
                'is_correct': is_correct,
                'classification_scores': classification_result.get('all_scores', {}),
                'method': method
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ æ¸¬è©¦ '{test_text}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                'test_text': test_text,
                'true_category': true_category,
                'predicted_category': 'ERROR',
                'final_category': 'ERROR',
                'is_correct': False,
                'error': str(e),
                'method': method
            }
    
    def run_comprehensive_test(self, test_ratio=0.3, num_variations=2, methods=['average']):
        """
        åŸ·è¡Œå…¨é¢æ¸¬è©¦
        Args:
            test_ratio: æ¸¬è©¦æ•¸æ“šæ¯”ä¾‹
            num_variations: æ¯å€‹è³‡ç”¢çš„è®ŠåŒ–ç‰ˆæœ¬æ•¸é‡
            methods: è¦æ¸¬è©¦çš„åˆ†é¡æ–¹æ³•åˆ—è¡¨
        Returns:
            dict: æ¸¬è©¦çµæœçµ±è¨ˆ
        """
        if self.test_data.empty:
            print("âŒ ç„¡æ³•åŸ·è¡Œæ¸¬è©¦ï¼šæ¸¬è©¦æ•¸æ“šç‚ºç©º")
            return {}
        
        print("="*100)
        print("ğŸ§ª é–‹å§‹åŸ·è¡Œå…¨é¢æº–ç¢ºæ€§æ¸¬è©¦")
        print("="*100)
        
        # æŒ‰é¡åˆ¥åˆ†å±¤æŠ½æ¨£
        test_cases = []
        category_groups = self.test_data.groupby('è³‡ç”¢é¡åˆ¥')
        
        print("ğŸ“‹ æ¸¬è©¦æ•¸æ“šæº–å‚™:")
        print("-"*50)
        
        for category, group in category_groups:
            # è¨ˆç®—è©²é¡åˆ¥çš„æ¸¬è©¦æ¨£æœ¬æ•¸é‡
            category_test_size = max(1, int(len(group) * test_ratio))
            category_sample = group.sample(n=category_test_size, random_state=42)
            
            print(f"é¡åˆ¥ [{category}]: {len(group)} é … â†’ æ¸¬è©¦ {len(category_sample)} é …")
            
            # ç‚ºæ¯å€‹æ¨£æœ¬å‰µå»ºè®ŠåŒ–ç‰ˆæœ¬
            for _, row in category_sample.iterrows():
                asset_name = row['è³‡ç”¢åç¨±']
                variations = self.create_test_variations(asset_name, num_variations)
                
                for variation in variations:
                    test_cases.append({
                        'test_text': variation,
                        'true_category': category,
                        'original_asset': asset_name,
                        'is_variation': variation != asset_name
                    })
        
        print(f"\nğŸ“Š ç¸½æ¸¬è©¦æ¡ˆä¾‹æ•¸é‡: {len(test_cases)}")
        print(f"ğŸ“Š åŸå§‹è³‡ç”¢: {len([case for case in test_cases if not case['is_variation']])}")
        print(f"ğŸ“Š è®ŠåŒ–ç‰ˆæœ¬: {len([case for case in test_cases if case['is_variation']])}")
        
        # éš¨æ©Ÿæ‰“äº‚æ¸¬è©¦é †åº
        random.shuffle(test_cases)
        
        # åŸ·è¡Œæ¸¬è©¦
        all_results = []
        
        for method in methods:
            print(f"\nğŸ” æ¸¬è©¦æ–¹æ³•: {method}")
            print("-"*50)
            
            method_results = []
            
            for i, test_case in enumerate(test_cases, 1):
                if i % 10 == 0 or i == len(test_cases):
                    print(f"é€²åº¦: {i}/{len(test_cases)} ({i/len(test_cases)*100:.1f}%)")
                
                result = self.test_single_classification(
                    test_case['test_text'],
                    test_case['true_category'],
                    method
                )
                
                # æ·»åŠ é¡å¤–ä¿¡æ¯
                result['original_asset'] = test_case['original_asset']
                result['is_variation'] = test_case['is_variation']
                
                method_results.append(result)
            
            all_results.extend(method_results)
        
        self.results = all_results
        
        # è¨ˆç®—å’Œé¡¯ç¤ºçµ±è¨ˆçµæœ
        stats = self.calculate_statistics()
        self.print_detailed_results(stats)
        
        return stats
    
    def calculate_statistics(self):
        """è¨ˆç®—çµ±è¨ˆçµæœ"""
        if not self.results:
            return {}
        
        # æŒ‰æ–¹æ³•åˆ†çµ„çµæœ
        method_stats = defaultdict(dict)
        
        for method in set(result['method'] for result in self.results):
            method_results = [r for r in self.results if r['method'] == method]
            
            # åŸºæœ¬æº–ç¢ºç‡çµ±è¨ˆ
            total_tests = len(method_results)
            correct_predictions = len([r for r in method_results if r['is_correct']])
            accuracy = correct_predictions / total_tests if total_tests > 0 else 0
            
            # æŒ‰é¡åˆ¥çµ±è¨ˆ
            category_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'tp': 0, 'fp': 0, 'fn': 0})
            
            for result in method_results:
                true_cat = result['true_category']
                pred_cat = result['final_category']
                
                category_stats[true_cat]['total'] += 1
                
                if result['is_correct']:
                    category_stats[true_cat]['correct'] += 1
                    category_stats[true_cat]['tp'] += 1
                else:
                    category_stats[true_cat]['fn'] += 1
                    if pred_cat != 'ERROR':
                        category_stats[pred_cat]['fp'] += 1
            
            # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1åˆ†æ•¸
            category_metrics = {}
            for category, stats in category_stats.items():
                precision = stats['tp'] / (stats['tp'] + stats['fp']) if (stats['tp'] + stats['fp']) > 0 else 0
                recall = stats['tp'] / (stats['tp'] + stats['fn']) if (stats['tp'] + stats['fn']) > 0 else 0
                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
                accuracy_per_cat = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
                
                category_metrics[category] = {
                    'total_samples': stats['total'],
                    'correct_predictions': stats['correct'],
                    'accuracy': accuracy_per_cat,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'true_positives': stats['tp'],
                    'false_positives': stats['fp'],
                    'false_negatives': stats['fn']
                }
            
            # è¨ˆç®—å®å¹³å‡å’Œå¾®å¹³å‡
            macro_precision = np.mean([metrics['precision'] for metrics in category_metrics.values()])
            macro_recall = np.mean([metrics['recall'] for metrics in category_metrics.values()])
            macro_f1 = np.mean([metrics['f1_score'] for metrics in category_metrics.values()])
            
            total_tp = sum(stats['tp'] for stats in category_stats.values())
            total_fp = sum(stats['fp'] for stats in category_stats.values())
            total_fn = sum(stats['fn'] for stats in category_stats.values())
            
            micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
            micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
            micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0
            
            # åŸå§‹è³‡ç”¢ vs è®ŠåŒ–ç‰ˆæœ¬çµ±è¨ˆ
            original_results = [r for r in method_results if not r['is_variation']]
            variation_results = [r for r in method_results if r['is_variation']]
            
            original_accuracy = len([r for r in original_results if r['is_correct']]) / len(original_results) if original_results else 0
            variation_accuracy = len([r for r in variation_results if r['is_correct']]) / len(variation_results) if variation_results else 0
            
            method_stats[method] = {
                'total_tests': total_tests,
                'correct_predictions': correct_predictions,
                'overall_accuracy': accuracy,
                'category_metrics': category_metrics,
                'macro_precision': macro_precision,
                'macro_recall': macro_recall,
                'macro_f1': macro_f1,
                'micro_precision': micro_precision,
                'micro_recall': micro_recall,
                'micro_f1': micro_f1,
                'original_tests': len(original_results),
                'original_accuracy': original_accuracy,
                'variation_tests': len(variation_results),
                'variation_accuracy': variation_accuracy
            }
        
        return dict(method_stats)
    
    def print_detailed_results(self, stats):
        """æ‰“å°è©³ç´°çµæœ"""
        print("\n" + "="*100)
        print("ğŸ“Š æ¸¬è©¦çµæœçµ±è¨ˆ")
        print("="*100)
        
        for method, method_stats in stats.items():
            print(f"\nğŸ” æ–¹æ³•: {method.upper()}")
            print("="*80)
            
            # ç¸½é«”æ€§èƒ½
            print(f"ğŸ“ˆ ç¸½é«”æ€§èƒ½:")
            print(f"   æ¸¬è©¦æ¡ˆä¾‹ç¸½æ•¸: {method_stats['total_tests']}")
            print(f"   æ­£ç¢ºé æ¸¬æ•¸é‡: {method_stats['correct_predictions']}")
            print(f"   ç¸½é«”æº–ç¢ºç‡: {method_stats['overall_accuracy']:.4f} ({method_stats['overall_accuracy']*100:.2f}%)")
            
            print(f"\nğŸ“Š å®å¹³å‡æŒ‡æ¨™:")
            print(f"   ç²¾ç¢ºç‡ (Precision): {method_stats['macro_precision']:.4f}")
            print(f"   å¬å›ç‡ (Recall): {method_stats['macro_recall']:.4f}")
            print(f"   F1 åˆ†æ•¸: {method_stats['macro_f1']:.4f}")
            
            print(f"\nğŸ“Š å¾®å¹³å‡æŒ‡æ¨™:")
            print(f"   ç²¾ç¢ºç‡ (Precision): {method_stats['micro_precision']:.4f}")
            print(f"   å¬å›ç‡ (Recall): {method_stats['micro_recall']:.4f}")
            print(f"   F1 åˆ†æ•¸: {method_stats['micro_f1']:.4f}")
            
            # åŸå§‹ vs è®ŠåŒ–ç‰ˆæœ¬æ€§èƒ½
            print(f"\nğŸ”„ åŸå§‹è³‡ç”¢ vs è®ŠåŒ–ç‰ˆæœ¬:")
            print(f"   åŸå§‹è³‡ç”¢æº–ç¢ºç‡: {method_stats['original_accuracy']:.4f} ({method_stats['original_tests']} å€‹æ¸¬è©¦)")
            print(f"   è®ŠåŒ–ç‰ˆæœ¬æº–ç¢ºç‡: {method_stats['variation_accuracy']:.4f} ({method_stats['variation_tests']} å€‹æ¸¬è©¦)")
            
            # å„é¡åˆ¥è©³ç´°æ€§èƒ½
            print(f"\nğŸ“‹ å„é¡åˆ¥è©³ç´°æ€§èƒ½:")
            print("-"*80)
            print(f"{'é¡åˆ¥':<15} {'æ¨£æœ¬æ•¸':<8} {'æº–ç¢ºç‡':<10} {'ç²¾ç¢ºç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•¸':<10}")
            print("-"*80)
            
            # æŒ‰æº–ç¢ºç‡æ’åº
            sorted_categories = sorted(
                method_stats['category_metrics'].items(),
                key=lambda x: x[1]['accuracy'],
                reverse=True
            )
            
            for category, metrics in sorted_categories:
                print(f"{category:<15} {metrics['total_samples']:<8} "
                      f"{metrics['accuracy']:.4f}   {metrics['precision']:.4f}   "
                      f"{metrics['recall']:.4f}   {metrics['f1_score']:.4f}")
    
    def analyze_error_cases(self, top_n=10):
        """åˆ†æéŒ¯èª¤æ¡ˆä¾‹"""
        if not self.results:
            print("âŒ æ²’æœ‰æ¸¬è©¦çµæœå¯ä¾›åˆ†æ")
            return
        
        print("\n" + "="*100)
        print("ğŸ” éŒ¯èª¤æ¡ˆä¾‹åˆ†æ")
        print("="*100)
        
        # æ‰¾å‡ºéŒ¯èª¤æ¡ˆä¾‹
        error_cases = [r for r in self.results if not r['is_correct']]
        
        if not error_cases:
            print("ğŸ‰ æ²’æœ‰éŒ¯èª¤æ¡ˆä¾‹ï¼æ‰€æœ‰é æ¸¬éƒ½æ­£ç¢ºã€‚")
            return
        
        print(f"ğŸ“Š éŒ¯èª¤æ¡ˆä¾‹ç¸½æ•¸: {len(error_cases)}")
        print(f"ğŸ“Š éŒ¯èª¤ç‡: {len(error_cases)/len(self.results)*100:.2f}%")
        
        # æŒ‰é¡åˆ¥çµ±è¨ˆéŒ¯èª¤
        error_by_category = defaultdict(list)
        for error in error_cases:
            error_by_category[error['true_category']].append(error)
        
        print(f"\nğŸ“‹ å„é¡åˆ¥éŒ¯èª¤çµ±è¨ˆ:")
        print("-"*60)
        for category, errors in error_by_category.items():
            print(f"{category}: {len(errors)} å€‹éŒ¯èª¤")
        
        # é¡¯ç¤ºæœ€åš´é‡çš„éŒ¯èª¤æ¡ˆä¾‹
        print(f"\nğŸ” Top {min(top_n, len(error_cases))} éŒ¯èª¤æ¡ˆä¾‹:")
        print("-"*100)
        
        for i, error in enumerate(error_cases[:top_n], 1):
            print(f"\nâŒ éŒ¯èª¤æ¡ˆä¾‹ #{i}")
            print(f"   æ¸¬è©¦æ–‡æœ¬: {error['test_text']}")
            print(f"   çœŸå¯¦é¡åˆ¥: {error['true_category']}")
            print(f"   é æ¸¬é¡åˆ¥: {error['final_category']}")
            if error.get('most_similar_asset'):
                print(f"   æœ€ç›¸ä¼¼è³‡ç”¢: {error['most_similar_asset']} (ç›¸ä¼¼åº¦: {error.get('similarity_score', 0):.4f})")
            if error.get('is_variation'):
                print(f"   åŸå§‹è³‡ç”¢: {error.get('original_asset', 'N/A')}")
            print("-"*50)
    
    def save_results(self, filename=None):
        """ä¿å­˜æ¸¬è©¦çµæœåˆ°æ–‡ä»¶"""
        if not self.results:
            print("âŒ æ²’æœ‰çµæœå¯ä¿å­˜")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"accuracy_test_results_{timestamp}.json"
        
        # æº–å‚™ä¿å­˜çš„æ•¸æ“š
        save_data = {
            'test_info': {
                'timestamp': datetime.now().isoformat(),
                'total_tests': len(self.results),
                'data_file': self.ra_data_path
            },
            'results': self.results,
            'statistics': self.calculate_statistics()
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ¸¬è©¦çµæœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜çµæœå¤±æ•—: {e}")
    
    def quick_test(self, num_samples=20):
        """å¿«é€Ÿæ¸¬è©¦ - éš¨æ©Ÿé¸æ“‡å°‘é‡æ¨£æœ¬é€²è¡Œæ¸¬è©¦"""
        if self.test_data.empty:
            print("âŒ ç„¡æ³•åŸ·è¡Œæ¸¬è©¦ï¼šæ¸¬è©¦æ•¸æ“šç‚ºç©º")
            return
        
        print("="*80)
        print("âš¡ å¿«é€Ÿæº–ç¢ºæ€§æ¸¬è©¦")
        print("="*80)
        
        # éš¨æ©Ÿé¸æ“‡æ¨£æœ¬
        sample_data = self.test_data.sample(n=min(num_samples, len(self.test_data)), random_state=42)
        
        print(f"ğŸ“Š æ¸¬è©¦æ¨£æœ¬æ•¸é‡: {len(sample_data)}")
        print("-"*50)
        
        results = []
        correct_count = 0
        
        for i, (_, row) in enumerate(sample_data.iterrows(), 1):
            asset_name = row['è³‡ç”¢åç¨±']
            true_category = row['è³‡ç”¢é¡åˆ¥']
            
            print(f"æ¸¬è©¦ {i}/{len(sample_data)}: {asset_name}")
            
            result = self.test_single_classification(asset_name, true_category)
            results.append(result)
            
            if result['is_correct']:
                correct_count += 1
                print(f"   âœ… æ­£ç¢º - é æ¸¬: {result['final_category']}")
            else:
                print(f"   âŒ éŒ¯èª¤ - çœŸå¯¦: {true_category}, é æ¸¬: {result['final_category']}")
        
        accuracy = correct_count / len(results)
        
        print("\n" + "="*50)
        print("ğŸ“Š å¿«é€Ÿæ¸¬è©¦çµæœ")
        print("="*50)
        print(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {len(results)}")
        print(f"æ­£ç¢ºé æ¸¬æ•¸: {correct_count}")
        print(f"æº–ç¢ºç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        return results

def main():
    """ä¸»ç¨‹å¼"""
    print("="*100)
    print("ğŸ¯ è³‡ç”¢åˆ†é¡ç³»çµ±æº–ç¢ºæ€§æ¸¬è©¦")
    print("="*100)
    print("æ­¤ç¨‹å¼æœƒæ¸¬è©¦åˆ†é¡ç³»çµ±çš„æº–ç¢ºæ€§ï¼ŒåŒ…æ‹¬:")
    print("1. åŸå§‹è³‡ç”¢åç¨±çš„åˆ†é¡æº–ç¢ºåº¦")
    print("2. è³‡ç”¢åç¨±è®ŠåŒ–ç‰ˆæœ¬çš„åˆ†é¡æº–ç¢ºåº¦") 
    print("3. å„é¡åˆ¥çš„ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1åˆ†æ•¸")
    print("4. éŒ¯èª¤æ¡ˆä¾‹åˆ†æ")
    print("="*100)
    
    # åˆå§‹åŒ–æ¸¬è©¦å™¨
    tester = AccuracyTester()
    
    while True:
        print("\nè«‹é¸æ“‡æ¸¬è©¦æ¨¡å¼:")
        print("1. å¿«é€Ÿæ¸¬è©¦ (20å€‹éš¨æ©Ÿæ¨£æœ¬)")
        print("2. ä¸­ç­‰æ¸¬è©¦ (50å€‹éš¨æ©Ÿæ¨£æœ¬)")
        print("3. å…¨é¢æ¸¬è©¦ (30%æ•¸æ“šï¼ŒåŒ…å«è®ŠåŒ–ç‰ˆæœ¬)")
        print("4. è‡ªå®šç¾©æ¸¬è©¦")
        print("5. åˆ†æä¸Šæ¬¡æ¸¬è©¦çš„éŒ¯èª¤æ¡ˆä¾‹")
        print("6. é€€å‡º")
        
        choice = input("\nè«‹é¸æ“‡ (1-6): ").strip()
        
        if choice == '1':
            print("\nğŸš€ åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦...")
            tester.quick_test(20)
            
        elif choice == '2':
            print("\nğŸš€ åŸ·è¡Œä¸­ç­‰æ¸¬è©¦...")
            tester.quick_test(50)
            
        elif choice == '3':
            print("\nğŸš€ åŸ·è¡Œå…¨é¢æ¸¬è©¦...")
            stats = tester.run_comprehensive_test(
                test_ratio=0.3, 
                num_variations=2, 
                methods=['average']
            )
            
            # è©¢å•æ˜¯å¦ä¿å­˜çµæœ
            save_choice = input("\næ˜¯å¦ä¿å­˜æ¸¬è©¦çµæœï¼Ÿ(y/n): ").strip().lower()
            if save_choice in ['y', 'yes', 'æ˜¯']:
                tester.save_results()
            
        elif choice == '4':
            try:
                test_ratio = float(input("è«‹è¼¸å…¥æ¸¬è©¦æ•¸æ“šæ¯”ä¾‹ (0.1-1.0): "))
                num_variations = int(input("è«‹è¼¸å…¥æ¯å€‹è³‡ç”¢çš„è®ŠåŒ–ç‰ˆæœ¬æ•¸é‡ (1-5): "))
                
                if 0.1 <= test_ratio <= 1.0 and 1 <= num_variations <= 5:
                    print(f"\nğŸš€ åŸ·è¡Œè‡ªå®šç¾©æ¸¬è©¦ (æ¯”ä¾‹: {test_ratio}, è®ŠåŒ–æ•¸: {num_variations})...")
                    stats = tester.run_comprehensive_test(
                        test_ratio=test_ratio,
                        num_variations=num_variations,
                        methods=['average']
                    )
                    
                    save_choice = input("\næ˜¯å¦ä¿å­˜æ¸¬è©¦çµæœï¼Ÿ(y/n): ").strip().lower()
                    if save_choice in ['y', 'yes', 'æ˜¯']:
                        tester.save_results()
                else:
                    print("âŒ åƒæ•¸ç¯„åœéŒ¯èª¤")
            except ValueError:
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")
        
        elif choice == '5':
            if tester.results:
                top_n = input("è«‹è¼¸å…¥è¦é¡¯ç¤ºçš„éŒ¯èª¤æ¡ˆä¾‹æ•¸é‡ (é è¨­10): ").strip()
                try:
                    top_n = int(top_n) if top_n else 10
                    tester.analyze_error_cases(top_n)
                except ValueError:
                    tester.analyze_error_cases(10)
            else:
                print("âŒ æ²’æœ‰æ¸¬è©¦çµæœï¼Œè«‹å…ˆåŸ·è¡Œæ¸¬è©¦")
        
        elif choice == '6':
            print("æ„Ÿè¬ä½¿ç”¨æº–ç¢ºæ€§æ¸¬è©¦å·¥å…·ï¼")
            break
        
        else:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„é¸é … (1-6)")

if __name__ == "__main__":
    main()