#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èª¿è©¦ç‰ˆæœ¬ - æª¢æŸ¥ä¿ç•™è©åˆ†é¡å•é¡Œ
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

from ultimate_classifier_with_reserved_words import UltimateClassifier

def debug_classification():
    """èª¿è©¦åˆ†é¡å•é¡Œ"""
    print("="*80)
    print("ğŸ” èª¿è©¦ä¿ç•™è©åˆ†é¡å•é¡Œ")
    print("="*80)
    
    classifier = UltimateClassifier()
    
    # å•é¡Œæ¡ˆä¾‹
    problem_cases = [
        ("é˜²ç«ç‰†è¨­å‚™", "å¯¦é«”"),
        ("ç›£æ§è¨­å‚™", "å¯¦é«”")
    ]
    
    for test_text, expected in problem_cases:
        print(f"\nğŸ¯ èª¿è©¦æ¡ˆä¾‹: '{test_text}' (æœŸæœ›: {expected})")
        print("-" * 60)
        
        # 1. æª¢æŸ¥æ–‡æœ¬è™•ç†
        processed = classifier.process_text_with_reserved_words(test_text)
        print(f"ğŸ“ æ–‡æœ¬è™•ç†çµæœ:")
        print(f"   åŸå§‹æ–‡æœ¬: {processed['original']}")
        print(f"   ä¿ç•™è©: {processed['reserved_words']}")
        print(f"   è©å½™å±•é–‹: {processed['expanded_tokens']}")
        print(f"   è™•ç†å¾Œæ–‡æœ¬: '{processed['processed_text']}'")
        print()
        
        # 2. æª¢æŸ¥æ¯å€‹é¡åˆ¥çš„è©³ç´°åˆ†æ•¸
        print(f"ğŸ“Š å„é¡åˆ¥åˆ†æ•¸è©³ç´°è¨ˆç®—:")
        categories = classifier.data['è³‡ç”¢é¡åˆ¥'].unique()
        
        for category in categories:
            print(f"\nğŸ”¸ {category} é¡åˆ¥:")
            rules = classifier.category_rules.get(category, {})
            
            # ä¿ç•™è©åˆ†æ•¸
            reserved_score = 0.0
            if 'reserved_boost' in rules:
                reserved_matches = []
                for boost_word in rules['reserved_boost']:
                    if boost_word in processed['reserved_words']:
                        reserved_score += 3.0
                        reserved_matches.append(boost_word)
                print(f"   ä¿ç•™è©åˆ†æ•¸: {reserved_score:.3f} (åŒ¹é…: {reserved_matches})")
            else:
                print(f"   ä¿ç•™è©åˆ†æ•¸: {reserved_score:.3f} (ç„¡ä¿ç•™è©è¦å‰‡)")
            
            # é—œéµè©åˆ†æ•¸
            keyword_score = 0.0
            keyword_matches = []
            all_text = processed['processed_text'].lower()
            for keyword in rules.get('keywords', []):
                if keyword.lower() in all_text:
                    keyword_score += 1.0
                    keyword_matches.append(keyword)
            print(f"   é—œéµè©åˆ†æ•¸: {keyword_score:.3f} (åŒ¹é…: {keyword_matches})")
            
            # æ¨¡å¼åˆ†æ•¸
            pattern_score = 0.0
            pattern_matches = []
            import re
            for pattern in rules.get('patterns', []):
                if re.search(pattern, processed['original'], re.IGNORECASE):
                    pattern_score += 1.0
                    pattern_matches.append(pattern)
            print(f"   æ¨¡å¼åˆ†æ•¸: {pattern_score:.3f} (åŒ¹é…: {pattern_matches})")
            
            # å‘é‡ç›¸ä¼¼åº¦
            similarity_score = 0.0
            if classifier.vectorizer and category in classifier.category_vectors:
                try:
                    input_vector = classifier.vectorizer.transform([processed['processed_text']])
                    from sklearn.metrics.pairwise import cosine_similarity
                    similarity = cosine_similarity(input_vector, classifier.category_vectors[category])[0, 0]
                    similarity_score = float(similarity)
                except Exception as e:
                    print(f"   å‘é‡è¨ˆç®—éŒ¯èª¤: {e}")
            print(f"   ç›¸ä¼¼åº¦åˆ†æ•¸: {similarity_score:.3f}")
            
            # æ’é™¤æ‡²ç½°
            exclude_penalty = 0.0
            if 'exclude_if_has_reserved' in rules:
                excluded = []
                for exclude_word in rules['exclude_if_has_reserved']:
                    if exclude_word in processed['reserved_words']:
                        exclude_penalty = 2.0
                        excluded.append(exclude_word)
                print(f"   æ’é™¤æ‡²ç½°: {exclude_penalty:.3f} (æ’é™¤è©: {excluded})")
            else:
                print(f"   æ’é™¤æ‡²ç½°: {exclude_penalty:.3f}")
            
            # æœ€çµ‚åˆ†æ•¸
            final_score = (
                reserved_score * 0.4 +
                keyword_score * 0.25 +
                pattern_score * 0.2 +
                similarity_score * 0.15
            ) - exclude_penalty
            
            final_score = max(0.0, final_score)
            
            print(f"   ğŸ† æœ€çµ‚åˆ†æ•¸: {final_score:.3f}")
            print(f"      = ({reserved_score:.3f}*0.4 + {keyword_score:.3f}*0.25 + {pattern_score:.3f}*0.2 + {similarity_score:.3f}*0.15) - {exclude_penalty:.3f}")
        
        # 3. åŸ·è¡Œåˆ†é¡
        result = classifier.classify(test_text)
        predicted = result['predicted_category']
        confidence = result['confidence']
        
        print(f"\nğŸ¯ åˆ†é¡çµæœ:")
        print(f"   é æ¸¬é¡åˆ¥: {predicted}")
        print(f"   ä¿¡å¿ƒåº¦: {confidence:.3f}")
        print(f"   æ˜¯å¦æ­£ç¢º: {'âœ…' if predicted == expected else 'âŒ'}")
        
        print(f"\nğŸ“‹ æ‰€æœ‰é¡åˆ¥æ’åº:")
        for i, (cat, score) in enumerate(result['sorted_scores'][:3], 1):
            print(f"   {i}. {cat}: {score:.3f}")

def check_rules():
    """æª¢æŸ¥è¦å‰‡é…ç½®"""
    print("\n" + "="*80)
    print("ğŸ”§ æª¢æŸ¥è¦å‰‡é…ç½®")
    print("="*80)
    
    classifier = UltimateClassifier()
    
    # æª¢æŸ¥å¯¦é«”é¡åˆ¥çš„è¦å‰‡ (æ‚¨çš„è³‡æ–™é›†ä¸­è¨­å‚™å±¬æ–¼å¯¦é«”é¡åˆ¥)
    entity_rules = classifier.category_rules.get('å¯¦é«”', {})
    print("ğŸ¢ å¯¦é«”é¡åˆ¥è¦å‰‡:")
    print(f"   é—œéµè©: {entity_rules.get('keywords', [])}")
    print(f"   æ¨¡å¼: {entity_rules.get('patterns', [])}")
    print(f"   ä¿ç•™è©åŠ æˆ: {entity_rules.get('reserved_boost', [])}")
    
    # æª¢æŸ¥è³‡æ–™é¡åˆ¥çš„è¦å‰‡
    data_rules = classifier.category_rules.get('è³‡æ–™', {})
    print("\nğŸ“„ è³‡æ–™é¡åˆ¥è¦å‰‡:")
    print(f"   é—œéµè©: {data_rules.get('keywords', [])}")
    print(f"   æ¨¡å¼: {data_rules.get('patterns', [])}")
    print(f"   ä¿ç•™è©åŠ æˆ: {data_rules.get('reserved_boost', [])}")

if __name__ == "__main__":
    debug_classification()
    check_rules()