#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·èªæ„åˆ†æå™¨ - æ”¯æ´å¤šç¨®èªæ„ç›¸ä¼¼åº¦è¨ˆç®—æ–¹æ³•
"""

import pandas as pd
import jieba
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

class EnhancedSemanticAnalyzer:
    """å¢å¼·ç‰ˆèªæ„åˆ†æå™¨"""
    
    def __init__(self, csv_path='RA_data.csv', use_sentence_transformer=False):
        """
        åˆå§‹åŒ–èªæ„åˆ†æå™¨
        Args:
            csv_path: CSVè³‡æ–™æª”æ¡ˆè·¯å¾‘
            use_sentence_transformer: æ˜¯å¦ä½¿ç”¨ Sentence Transformers (éœ€é¡å¤–å®‰è£)
        """
        self.csv_path = csv_path
        self.use_sentence_transformer = use_sentence_transformer
        self.data = None
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.sentence_model = None
        self.sentence_embeddings = None
        
        # è¼‰å…¥è³‡æ–™
        self.load_data()
        
        # åˆå§‹åŒ– TF-IDF
        self.setup_tfidf()
        
        # åˆå§‹åŒ– Sentence Transformer (å¯é¸)
        if use_sentence_transformer:
            self.setup_sentence_transformer()
    
    def load_data(self):
        """è¼‰å…¥CSVè³‡æ–™"""
        try:
            self.data = pd.read_csv(self.csv_path, encoding='utf-8')
            print(f"è¼‰å…¥äº† {len(self.data)} ç­†è³‡æ–™")
        except Exception as e:
            print(f"è¼‰å…¥è³‡æ–™å¤±æ•—: {e}")
            self.data = pd.DataFrame()
    
    def preprocess_text(self, text):
        """æ–‡æœ¬é è™•ç†"""
        if pd.isna(text):
            return ""
        
        # æ¸…ç†æ–‡æœ¬
        text = str(text).strip()
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
        
        # jieba åˆ†è©
        words = jieba.cut(text)
        processed = ' '.join([word.strip() for word in words if len(word.strip()) > 0])
        
        return processed
    
    def setup_tfidf(self):
        """è¨­ç½® TF-IDF å‘é‡åŒ–"""
        if self.data.empty:
            return
        
        # é è™•ç†æ‰€æœ‰è³‡ç”¢åç¨±
        processed_names = [self.preprocess_text(name) for name in self.data['è³‡ç”¢åç¨±']]
        
        # å»ºç«‹ TF-IDF å‘é‡åŒ–å™¨
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 3),  # å¢åŠ  n-gram ç¯„åœ
            min_df=1,
            max_df=0.8
        )
        
        # è¨ˆç®— TF-IDF çŸ©é™£
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_names)
        print(f"TF-IDF çŸ©é™£å½¢ç‹€: {self.tfidf_matrix.shape}")
    
    def setup_sentence_transformer(self):
        """è¨­ç½® Sentence Transformer (éœ€è¦å®‰è£ sentence-transformers)"""
        try:
            # ä½¿ç”¨å¤šèªè¨€æ¨¡å‹
            self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            
            # è¨ˆç®—æ‰€æœ‰è³‡ç”¢åç¨±çš„å¥å­åµŒå…¥
            asset_names = self.data['è³‡ç”¢åç¨±'].tolist()
            self.sentence_embeddings = self.sentence_model.encode(asset_names)
            print(f"Sentence embeddings å½¢ç‹€: {self.sentence_embeddings.shape}")
            
        except ImportError:
            print("âŒ è«‹å®‰è£ sentence-transformers: pip install sentence-transformers")
            self.use_sentence_transformer = False
        except Exception as e:
            print(f"âŒ Sentence Transformer åˆå§‹åŒ–å¤±æ•—: {e}")
            self.use_sentence_transformer = False
    
    def tfidf_similarity(self, input_text, top_k=10):
        """ä½¿ç”¨ TF-IDF è¨ˆç®—ç›¸ä¼¼åº¦"""
        if self.tfidf_matrix is None:
            return []
        
        # é è™•ç†è¼¸å…¥æ–‡æœ¬
        processed_input = self.preprocess_text(input_text)
        
        # è½‰æ›ç‚º TF-IDF å‘é‡
        input_vector = self.tfidf_vectorizer.transform([processed_input])
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity(input_vector, self.tfidf_matrix).flatten()
        
        # ç²å–æœ€ç›¸ä¼¼çš„é …ç›®
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:
                results.append({
                    'method': 'TF-IDF',
                    'asset_name': self.data.iloc[idx]['è³‡ç”¢åç¨±'],
                    'category': self.data.iloc[idx]['è³‡ç”¢é¡åˆ¥'],
                    'similarity': float(similarities[idx]),
                    'index': int(idx)
                })
        
        return results
    
    def sentence_similarity(self, input_text, top_k=10):
        """ä½¿ç”¨ Sentence Transformer è¨ˆç®—ç›¸ä¼¼åº¦"""
        if not self.use_sentence_transformer or self.sentence_embeddings is None:
            return []
        
        try:
            # è¨ˆç®—è¼¸å…¥æ–‡æœ¬çš„åµŒå…¥
            input_embedding = self.sentence_model.encode([input_text])
            
            # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
            similarities = cosine_similarity(input_embedding, self.sentence_embeddings).flatten()
            
            # ç²å–æœ€ç›¸ä¼¼çš„é …ç›®
            top_indices = similarities.argsort()[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0:
                    results.append({
                        'method': 'Sentence Transformer',
                        'asset_name': self.data.iloc[idx]['è³‡ç”¢åç¨±'],
                        'category': self.data.iloc[idx]['è³‡ç”¢é¡åˆ¥'],
                        'similarity': float(similarities[idx]),
                        'index': int(idx)
                    })
            
            return results
            
        except Exception as e:
            print(f"Sentence similarity è¨ˆç®—å¤±æ•—: {e}")
            return []
    
    def fuzzy_keyword_matching(self, input_text, top_k=10):
        """åŸºæ–¼é—œéµè©çš„æ¨¡ç³ŠåŒ¹é…"""
        # æå–è¼¸å…¥æ–‡æœ¬çš„é—œéµè©
        input_words = set(jieba.cut(input_text))
        input_words = {word for word in input_words if len(word) > 1}
        
        results = []
        
        for idx, row in self.data.iterrows():
            asset_name = row['è³‡ç”¢åç¨±']
            asset_words = set(jieba.cut(asset_name))
            asset_words = {word for word in asset_words if len(word) > 1}
            
            # è¨ˆç®— Jaccard ç›¸ä¼¼åº¦
            intersection = len(input_words.intersection(asset_words))
            union = len(input_words.union(asset_words))
            
            if union > 0:
                jaccard_sim = intersection / union
                
                # é¡å¤–åŠ æ¬Šï¼šå¦‚æœæœ‰å®Œå…¨åŒ¹é…çš„é—œéµè©ï¼Œæé«˜åˆ†æ•¸
                exact_matches = sum(1 for word in input_words if word in asset_name)
                boost = exact_matches * 0.2
                
                final_score = min(jaccard_sim + boost, 1.0)
                
                if final_score > 0:
                    results.append({
                        'method': 'Fuzzy Keywords',
                        'asset_name': asset_name,
                        'category': row['è³‡ç”¢é¡åˆ¥'],
                        'similarity': final_score,
                        'index': int(idx),
                        'matched_words': list(input_words.intersection(asset_words))
                    })
        
        # æ’åºä¸¦è¿”å› top-k
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    def analyze_comprehensive(self, input_text, top_k=10, methods=['tfidf', 'fuzzy']):
        """
        ç¶œåˆåˆ†æï¼šçµåˆå¤šç¨®æ–¹æ³•
        Args:
            input_text: è¼¸å…¥æ–‡æœ¬
            top_k: è¿”å›çµæœæ•¸é‡
            methods: ä½¿ç”¨çš„æ–¹æ³•åˆ—è¡¨ ['tfidf', 'sentence', 'fuzzy']
        """
        all_results = []
        
        print(f"æ­£åœ¨åˆ†æ: {input_text}")
        print("-" * 50)
        
        # TF-IDF æ–¹æ³•
        if 'tfidf' in methods:
            tfidf_results = self.tfidf_similarity(input_text, top_k)
            all_results.extend(tfidf_results)
            print(f"TF-IDF æ‰¾åˆ° {len(tfidf_results)} å€‹ç›¸ä¼¼é …ç›®")
        
        # Sentence Transformer æ–¹æ³•
        if 'sentence' in methods and self.use_sentence_transformer:
            sentence_results = self.sentence_similarity(input_text, top_k)
            all_results.extend(sentence_results)
            print(f"Sentence Transformer æ‰¾åˆ° {len(sentence_results)} å€‹ç›¸ä¼¼é …ç›®")
        
        # æ¨¡ç³Šé—œéµè©åŒ¹é…
        if 'fuzzy' in methods:
            fuzzy_results = self.fuzzy_keyword_matching(input_text, top_k)
            all_results.extend(fuzzy_results)
            print(f"æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ° {len(fuzzy_results)} å€‹ç›¸ä¼¼é …ç›®")
        
        # åˆä½µå’Œå»é‡çµæœ
        combined_results = self.combine_results(all_results, top_k)
        
        return combined_results, self.preprocess_text(input_text)
    
    def combine_results(self, all_results, top_k):
        """åˆä½µå¤šç¨®æ–¹æ³•çš„çµæœï¼Œä½¿ç”¨åŠ æ¬Šå¹³å‡"""
        # æŒ‰è³‡ç”¢åç¨±åˆ†çµ„
        asset_groups = {}
        
        for result in all_results:
            asset_name = result['asset_name']
            if asset_name not in asset_groups:
                asset_groups[asset_name] = {
                    'asset_name': asset_name,
                    'category': result['category'],
                    'similarities': [],
                    'methods': []
                }
            
            asset_groups[asset_name]['similarities'].append(result['similarity'])
            asset_groups[asset_name]['methods'].append(result['method'])
        
        # è¨ˆç®—ç¶œåˆåˆ†æ•¸
        final_results = []
        for asset_name, group in asset_groups.items():
            # ä½¿ç”¨æœ€é«˜åˆ†æ•¸ + å¹³å‡åˆ†æ•¸çš„åŠ æ¬Šçµ„åˆ
            similarities = group['similarities']
            max_sim = max(similarities)
            avg_sim = np.mean(similarities)
            
            # å¦‚æœå¤šå€‹æ–¹æ³•éƒ½æ‰¾åˆ°äº†é€™å€‹é …ç›®ï¼Œçµ¦äºˆé¡å¤–æ¬Šé‡
            method_bonus = len(similarities) * 0.1
            
            combined_score = 0.6 * max_sim + 0.3 * avg_sim + method_bonus
            combined_score = min(combined_score, 1.0)
            
            final_results.append({
                'asset_name': asset_name,
                'category': group['category'],
                'similarity': combined_score,
                'methods_used': group['methods'],
                'individual_scores': similarities
            })
        
        # æ’åºä¸¦è¿”å› top-k
        final_results.sort(key=lambda x: x['similarity'], reverse=True)
        return final_results[:top_k]

def test_enhanced_semantic():
    """æ¸¬è©¦å¢å¼·èªæ„åˆ†æ"""
    print("="*80)
    print("ğŸ§ª å¢å¼·èªæ„åˆ†ææ¸¬è©¦")
    print("="*80)
    
    # å‰µå»ºåˆ†æå™¨ (å…ˆä¸ä½¿ç”¨ Sentence Transformer)
    analyzer = EnhancedSemanticAnalyzer('RA_data.csv', use_sentence_transformer=False)
    
    test_cases = [
        "MySQL è³‡æ–™åº«",
        "Windows ä½œæ¥­ç³»çµ±",
        "ç³»çµ±ç®¡ç†å“¡",
        "å‚™ä»½æª”æ¡ˆ",
        "é˜²ç«ç‰†",
        "é›²ç«¯æœå‹™å™¨",
        "ç¶²è·¯è¨­å‚™"
    ]
    
    for test_text in test_cases:
        print(f"\n{'='*60}")
        print(f"ğŸ” æ¸¬è©¦æ¡ˆä¾‹: {test_text}")
        print('='*60)
        
        # ç¶œåˆåˆ†æ
        results, processed = analyzer.analyze_comprehensive(
            test_text, 
            top_k=5, 
            methods=['tfidf', 'fuzzy']
        )
        
        print(f"è™•ç†å¾Œæ–‡æœ¬: {processed}")
        print(f"\nğŸ“Š ç¶œåˆçµæœ (Top 5):")
        print("-" * 50)
        
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['asset_name']}")
            print(f"   é¡åˆ¥: {result['category']}")
            print(f"   ç¶œåˆåˆ†æ•¸: {result['similarity']:.4f}")
            print(f"   ä½¿ç”¨æ–¹æ³•: {', '.join(result['methods_used'])}")
            print(f"   å€‹åˆ¥åˆ†æ•¸: {[f'{s:.3f}' for s in result['individual_scores']]}")
            print()

if __name__ == "__main__":
    test_enhanced_semantic()