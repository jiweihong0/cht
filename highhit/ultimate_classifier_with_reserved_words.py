#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ‚æ¥µå„ªåŒ–åˆ†é¡å™¨ - æ•´åˆä¿ç•™è©åŠŸèƒ½
é‡å°æ‚¨æåˆ°çš„å•é¡Œé€²è¡Œæœ€çµ‚å„ªåŒ–
"""

import pandas as pd
import numpy as np
import re
import json
from datetime import datetime
from collections import defaultdict, Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
import jieba.posseg as pseg

class UltimateClassifier:
    """çµ‚æ¥µå„ªåŒ–åˆ†é¡å™¨"""
    
    def __init__(self, data_path='RA_data.csv'):
        """åˆå§‹åŒ–çµ‚æ¥µåˆ†é¡å™¨"""
        self.data_path = data_path
        self.data = None
        
        # ä¿ç•™è©å­—å…¸ - è§£æ±ºæ‚¨çš„æ ¸å¿ƒå•é¡Œ
        self.reserved_words = {
            # æ ¸å¿ƒå•é¡Œï¼šé˜²ç«ç‰†è¨­å‚™é¡
            'é˜²ç«ç‰†è¨­å‚™': ['é˜²ç«ç‰†', 'è¨­å‚™'],
            'ç¶²è·¯è¨­å‚™': ['ç¶²è·¯', 'è¨­å‚™'],
            'å„²å­˜è¨­å‚™': ['å„²å­˜', 'è¨­å‚™'],
            'ç›£æ§è¨­å‚™': ['ç›£æ§', 'è¨­å‚™'],
            'å®‰å…¨è¨­å‚™': ['å®‰å…¨', 'è¨­å‚™'],
            
            # è³‡æ–™åº«ç›¸é—œ
            'è³‡æ–™åº«ç®¡ç†ç³»çµ±': ['è³‡æ–™åº«', 'ç®¡ç†ç³»çµ±'],
            'è³‡æ–™åº«ç³»çµ±': ['è³‡æ–™åº«', 'ç³»çµ±'],
            'ç®¡ç†ç³»çµ±': ['ç®¡ç†', 'ç³»çµ±'],
            
            # äººå“¡ç›¸é—œ
            'å…§éƒ¨äººå“¡': ['å…§éƒ¨', 'äººå“¡'],
            'å¤–éƒ¨äººå“¡': ['å¤–éƒ¨', 'äººå“¡'],
            'ç³»çµ±ç®¡ç†å“¡': ['ç³»çµ±', 'ç®¡ç†å“¡'],
            
            # æ–‡ä»¶ç›¸é—œ  
            'ä½œæ¥­æ–‡ä»¶': ['ä½œæ¥­', 'æ–‡ä»¶'],
            'é›»å­ç´€éŒ„': ['é›»å­', 'ç´€éŒ„'],
            'ç¨‹åºæ–‡ä»¶': ['ç¨‹åº', 'æ–‡ä»¶'],
            'æŠ€è¡“æ–‡ä»¶': ['æŠ€è¡“', 'æ–‡ä»¶'],
            
            # æœå‹™ç›¸é—œ
            'ç¶²è·¯æœå‹™': ['ç¶²è·¯', 'æœå‹™'],
            'é›²ç«¯æœå‹™': ['é›²ç«¯', 'æœå‹™'],
            'æ‡‰ç”¨æœå‹™': ['æ‡‰ç”¨', 'æœå‹™'],
            
            # å¯¦é«”ç›¸é—œ
            'å¯æ”œå¼å„²å­˜åª’é«”': ['å¯æ”œå¼', 'å„²å­˜åª’é«”'],
            'å„²å­˜åª’é«”': ['å„²å­˜', 'åª’é«”'],
            
            # ç³»çµ±ç›¸é—œ
            'ä½œæ¥­ç³»çµ±': ['ä½œæ¥­ç³»çµ±'],
            'Windows': ['Windows'],
            'Linux': ['Linux'],
            'MySQL': ['MySQL'],
            'Oracle': ['Oracle']
        }
        
        # è¨»å†Šä¿ç•™è©
        self._register_reserved_words()
        
        # å…¶ä»–çµ„ä»¶
        self.category_rules = {}
        self.vectorizer = None
        self.category_vectors = {}
        
        # åˆå§‹åŒ–
        self.load_data()
        self.build_features()
        self.create_rules()
    
    def _register_reserved_words(self):
        """è¨»å†Šä¿ç•™è©åˆ° jieba"""
        for phrase in self.reserved_words.keys():
            jieba.add_word(phrase, freq=10000)
    
    def load_data(self):
        """è¼‰å…¥è³‡æ–™"""
        try:
            self.data = pd.read_csv(self.data_path, encoding='utf-8')
            print(f"âœ… è¼‰å…¥è³‡æ–™ï¼š{len(self.data)} ç­†è¨˜éŒ„")
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—ï¼š{e}")
            self.data = pd.DataFrame()
    
    def process_text_with_reserved_words(self, text):
        """ä½¿ç”¨ä¿ç•™è©è™•ç†æ–‡æœ¬"""
        if not isinstance(text, str):
            text = str(text)
        
        # æ‰¾å‡ºä¿ç•™è©
        found_reserved = []
        remaining_text = text
        
        # æŒ‰é•·åº¦æ’åºï¼Œå„ªå…ˆåŒ¹é…é•·è©
        sorted_reserved = sorted(self.reserved_words.keys(), key=len, reverse=True)
        
        for reserved_phrase in sorted_reserved:
            if reserved_phrase in text:
                found_reserved.append(reserved_phrase)
                remaining_text = remaining_text.replace(reserved_phrase, f' [RESERVED] ')
        
        # è™•ç†å‰©é¤˜æ–‡æœ¬
        remaining_words = []
        clean_remaining = re.sub(r'\[RESERVED\]', '', remaining_text).strip()
        if clean_remaining:
            remaining_words = [w for w in jieba.cut(clean_remaining) if len(w.strip()) > 1]
        
        # å±•é–‹ä¿ç•™è©
        expanded_tokens = []
        for reserved in found_reserved:
            if reserved in self.reserved_words:
                expanded_tokens.extend(self.reserved_words[reserved])
            else:
                expanded_tokens.append(reserved)
        
        # çµ„åˆæœ€çµ‚çµæœ
        all_tokens = expanded_tokens + remaining_words
        
        return {
            'original': text,
            'reserved_words': found_reserved,
            'remaining_words': remaining_words,
            'expanded_tokens': expanded_tokens,
            'all_tokens': all_tokens,
            'processed_text': ' '.join(all_tokens)
        }
    
    def build_features(self):
        """å»ºç«‹ç‰¹å¾µ"""
        if self.data.empty:
            return
        
        # è™•ç†æ‰€æœ‰è³‡ç”¢åç¨±
        category_texts = defaultdict(list)
        
        for _, row in self.data.iterrows():
            category = row['è³‡ç”¢é¡åˆ¥']
            asset_name = row['è³‡ç”¢åç¨±']
            
            # ä½¿ç”¨ä¿ç•™è©è™•ç†
            processed = self.process_text_with_reserved_words(asset_name)
            
            # æ”¶é›†æ–‡æœ¬è®ŠåŒ–
            category_texts[category].extend([
                processed['processed_text'],
                processed['original'],
                ' '.join(processed['remaining_words']),
                ' '.join(processed['expanded_tokens'])
            ])
        
        # å»ºç«‹å‘é‡åŒ–å™¨
        all_texts = []
        category_labels = []
        
        for category, texts in category_texts.items():
            for text in texts:
                if text.strip():
                    all_texts.append(text)
                    category_labels.append(category)
        
        if all_texts:
            self.vectorizer = TfidfVectorizer(
                analyzer='char_wb',
                ngram_range=(2, 4),
                max_features=8000,
                lowercase=True
            )
            
            vectors = self.vectorizer.fit_transform(all_texts)
            
            # è¨ˆç®—é¡åˆ¥å‘é‡
            for category in category_texts.keys():
                category_indices = [i for i, label in enumerate(category_labels) if label == category]
                if category_indices:
                    category_vector = vectors[category_indices].mean(axis=0)
                    # ç¢ºä¿æ˜¯ numpy array è€Œä¸æ˜¯ matrix
                    if hasattr(category_vector, 'A1'):
                        category_vector = category_vector.A1  # è½‰æ› matrix åˆ° array
                    self.category_vectors[category] = category_vector
    
    def create_rules(self):
        """å‰µå»ºåˆ†é¡è¦å‰‡"""
        self.category_rules = {
            'è»Ÿé«”': {
                'keywords': ['ç³»çµ±', 'è»Ÿé«”', 'æ‡‰ç”¨ç¨‹å¼', 'è³‡æ–™åº«', 'ç¨‹å¼', 'MySQL', 'Oracle', 'Windows', 'Linux', 'ç®¡ç†ç³»çµ±'],
                'patterns': [r'.*ç³»çµ±$', r'.*è»Ÿé«”$', r'.*ç¨‹å¼.*', r'.*è³‡æ–™åº«.*'],
                'reserved_boost': ['è³‡æ–™åº«ç®¡ç†ç³»çµ±', 'MySQL', 'Oracle', 'Windows', 'Linux', 'ç®¡ç†ç³»çµ±'],
                'weight': 1.0
            },
            'å¯¦é«”': {
                'keywords': ['è¨­å‚™', 'ç¡¬é«”', 'ä¼ºæœå™¨', 'ä¸»æ©Ÿ', 'é˜²ç«ç‰†', 'ç¶²è·¯', 'å„²å­˜', 'ç›£æ§', 'å¯¦é«”', 'ç’°å¢ƒ', 'è¨­æ–½', 'å ´æ‰€', 'æ©Ÿæˆ¿', 'åª’é«”', 'å¯æ”œå¼'],
                'patterns': [r'.*è¨­å‚™$', r'.*ä¸»æ©Ÿ$', r'.*ä¼ºæœå™¨.*', r'.*ç’°å¢ƒ.*', r'.*è¨­æ–½.*', r'.*å ´æ‰€.*'],
                'reserved_boost': ['é˜²ç«ç‰†è¨­å‚™', 'ç¶²è·¯è¨­å‚™', 'å„²å­˜è¨­å‚™', 'ç›£æ§è¨­å‚™', 'å®‰å…¨è¨­å‚™', 'å¯æ”œå¼å„²å­˜åª’é«”', 'å„²å­˜åª’é«”'],
                'weight': 1.0
            },
            'è³‡æ–™': {
                'keywords': ['è³‡æ–™', 'æ–‡ä»¶', 'æª”æ¡ˆ', 'ç´€éŒ„', 'åˆç´„', 'ä½œæ¥­', 'é›»å­', 'ç¨‹åº', 'æŠ€è¡“'],
                'patterns': [r'.*æ–‡ä»¶.*', r'.*æª”æ¡ˆ.*', r'.*ç´€éŒ„.*'],
                'reserved_boost': ['ä½œæ¥­æ–‡ä»¶', 'é›»å­ç´€éŒ„', 'ç¨‹åºæ–‡ä»¶', 'æŠ€è¡“æ–‡ä»¶'],
                'weight': 1.0
            },
            'äººå“¡': {
                'keywords': ['äººå“¡', 'å“¡å·¥', 'ä½¿ç”¨è€…', 'ç®¡ç†å“¡', 'å…§éƒ¨', 'å¤–éƒ¨'],
                'patterns': [r'.*äººå“¡$', r'.*å“¡å·¥.*', r'.*ç®¡ç†å“¡.*'],
                'reserved_boost': ['å…§éƒ¨äººå“¡', 'å¤–éƒ¨äººå“¡', 'ç³»çµ±ç®¡ç†å“¡'],
                'exclude_if_has_reserved': ['ä½œæ¥­æ–‡ä»¶', 'é›»å­ç´€éŒ„', 'ç¨‹åºæ–‡ä»¶'],
                'weight': 1.0
            },
            'æœå‹™': {
                'keywords': ['æœå‹™', 'æ‡‰ç”¨', 'ç¶²è·¯', 'é›²ç«¯', 'API'],
                'patterns': [r'.*æœå‹™.*', r'.*æ‡‰ç”¨.*'],
                'reserved_boost': ['ç¶²è·¯æœå‹™', 'é›²ç«¯æœå‹™', 'æ‡‰ç”¨æœå‹™'],
                'weight': 1.0
            }
        }
    
    def calculate_score(self, processed_text, category):
        """è¨ˆç®—åˆ†é¡åˆ†æ•¸"""
        if category not in self.category_rules:
            return 0.0
        
        rules = self.category_rules[category]
        score = 0.0
        
        # 1. ä¿ç•™è©åŠ æˆ (æœ€é«˜æ¬Šé‡)
        reserved_score = 0.0
        if 'reserved_boost' in rules:
            for boost_word in rules['reserved_boost']:
                if boost_word in processed_text['reserved_words']:
                    reserved_score += 3.0
        
        # 2. é—œéµè©åŒ¹é…
        keyword_score = 0.0
        all_text = processed_text['processed_text'].lower()
        for keyword in rules.get('keywords', []):
            if keyword.lower() in all_text:
                keyword_score += 1.0
        
        # 3. æ¨¡å¼åŒ¹é…
        pattern_score = 0.0
        for pattern in rules.get('patterns', []):
            if re.search(pattern, processed_text['original'], re.IGNORECASE):
                pattern_score += 1.0
        
        # 4. å‘é‡ç›¸ä¼¼åº¦
        similarity_score = 0.0
        if self.vectorizer and category in self.category_vectors:
            try:
                input_vector = self.vectorizer.transform([processed_text['processed_text']])
                # ç¢ºä¿å‘é‡æ˜¯æ­£ç¢ºçš„æ ¼å¼
                category_vector = self.category_vectors[category]
                if hasattr(category_vector, 'A1'):
                    category_vector = category_vector.A1
                if hasattr(input_vector, 'toarray'):
                    input_vector = input_vector.toarray()
                
                # è¨ˆç®—ç›¸ä¼¼åº¦
                if len(category_vector.shape) == 1:
                    category_vector = category_vector.reshape(1, -1)
                if len(input_vector.shape) == 1:
                    input_vector = input_vector.reshape(1, -1)
                    
                similarity = cosine_similarity(input_vector, category_vector)[0, 0]
                similarity_score = float(similarity)
            except Exception as e:
                similarity_score = 0.0
        
        # 5. æ’é™¤è¦å‰‡
        exclude_penalty = 0.0
        if 'exclude_if_has_reserved' in rules:
            for exclude_word in rules['exclude_if_has_reserved']:
                if exclude_word in processed_text['reserved_words']:
                    exclude_penalty = 2.0  # å¤§å¹…æ‰£åˆ†
        
        # åŠ æ¬Šçµ„åˆ
        final_score = (
            reserved_score * 0.4 +     # ä¿ç•™è© 40%
            keyword_score * 0.25 +     # é—œéµè© 25%
            pattern_score * 0.2 +      # æ¨¡å¼ 20%
            similarity_score * 0.15    # ç›¸ä¼¼åº¦ 15%
        ) - exclude_penalty
        
        return max(0.0, final_score)  # ç¢ºä¿éè² 
    
    def classify(self, input_text):
        """åˆ†é¡æ–‡æœ¬"""
        if self.data.empty:
            return {'error': 'æ²’æœ‰å¯ç”¨çš„è¨“ç·´è³‡æ–™'}
        
        # ä½¿ç”¨ä¿ç•™è©è™•ç†
        processed_text = self.process_text_with_reserved_words(input_text)
        
        # è¨ˆç®—æ‰€æœ‰é¡åˆ¥çš„åˆ†æ•¸
        categories = self.data['è³‡ç”¢é¡åˆ¥'].unique()
        category_scores = {}
        
        for category in categories:
            score = self.calculate_score(processed_text, category)
            category_scores[category] = score
        
        # æ‰¾å‡ºæœ€ä½³é æ¸¬
        best_category = max(category_scores.keys(), key=lambda x: category_scores[x])
        best_score = category_scores[best_category]
        
        # æ’åºåˆ†æ•¸
        sorted_scores = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)
        
        return {
            'input_text': input_text,
            'predicted_category': best_category,
            'confidence': best_score,
            'all_scores': category_scores,
            'sorted_scores': sorted_scores,
            'processed_info': processed_text
        }

def run_ultimate_test():
    """åŸ·è¡Œçµ‚æ¥µæ¸¬è©¦"""
    print("="*80)
    print("ğŸš€ çµ‚æ¥µå„ªåŒ–åˆ†é¡å™¨æ¸¬è©¦")
    print("="*80)
    print("é‡é»è§£æ±ºï¼šé˜²ç«ç‰†è¨­å‚™ â†’ é˜²ç«ç‰† + è¨­å‚™ (è€Œä¸æ˜¯ é˜²ç« + ç‰† + è¨­å‚™)")
    print()
    
    classifier = UltimateClassifier()
    
    # æ‚¨ç‰¹åˆ¥é—œå¿ƒçš„æ¸¬è©¦æ¡ˆä¾‹
    critical_cases = [
        ("é˜²ç«ç‰†è¨­å‚™", "å¯¦é«”", "æ‚¨çš„æ ¸å¿ƒå•é¡Œ"),
        ("è³‡æ–™åº«ç®¡ç†ç³»çµ±", "è»Ÿé«”", "è¤‡åˆç³»çµ±è©å½™"),
        ("å¯æ”œå¼å„²å­˜åª’é«”", "å¯¦é«”", "è¤‡é›œå¯¦é«”æè¿°"),
        ("å…§éƒ¨äººå“¡", "äººå“¡", "äººå“¡é¡åˆ¥"),
        ("å¤–éƒ¨äººå“¡", "äººå“¡", "äººå“¡é¡åˆ¥"),
        ("ä½œæ¥­æ–‡ä»¶", "è³‡æ–™", "å®¹æ˜“èª¤åˆ¤çš„æ–‡ä»¶"),
        ("é›»å­ç´€éŒ„", "è³‡æ–™", "å®¹æ˜“èª¤åˆ¤çš„ç´€éŒ„"),
        ("ç¶²è·¯æœå‹™", "æœå‹™", "æœå‹™é¡åˆ¥"),
        ("é›²ç«¯æœå‹™", "æœå‹™", "æœå‹™é¡åˆ¥"),
        ("MySQLè³‡æ–™åº«", "è»Ÿé«”", "å“ç‰Œ+é¡å‹"),
        ("Oracleç³»çµ±", "è»Ÿé«”", "å“ç‰Œç³»çµ±"),
        ("Windowsä½œæ¥­ç³»çµ±", "è»Ÿé«”", "ä½œæ¥­ç³»çµ±"),
        ("ç›£æ§è¨­å‚™", "å¯¦é«”", "è¨­å‚™é¡"),
        ("ç¨‹åºæ–‡ä»¶", "è³‡æ–™", "æ–‡ä»¶é¡")
    ]
    
    correct_count = 0
    total_count = len(critical_cases)
    
    print("ğŸ“‹ æ¸¬è©¦çµæœ:")
    print("-" * 80)
    
    for i, (test_text, expected, description) in enumerate(critical_cases, 1):
        result = classifier.classify(test_text)
        predicted = result['predicted_category']
        confidence = result['confidence']
        reserved_words = result['processed_info']['reserved_words']
        
        is_correct = predicted == expected
        if is_correct:
            correct_count += 1
            status = "âœ…"
        else:
            status = "âŒ"
        
        print(f"{status} {i:2d}. '{test_text}' â†’ {predicted} (æœŸæœ›: {expected})")
        print(f"       {description} | ä¿¡å¿ƒåº¦: {confidence:.3f}")
        
        if reserved_words:
            print(f"       ä¿ç•™è©: {reserved_words}")
        
        # é¡¯ç¤ºè™•ç†éç¨‹
        processed_info = result['processed_info']
        if processed_info['expanded_tokens']:
            print(f"       è©å½™å±•é–‹: {processed_info['expanded_tokens']}")
        
        print()
    
    # çµ±è¨ˆçµæœ
    accuracy = correct_count / total_count
    print("="*80)
    print("ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
    print(f"âœ… æ­£ç¢ºé æ¸¬: {correct_count}/{total_count}")
    print(f"ğŸ“ˆ æº–ç¢ºç‡: {accuracy:.3f} ({accuracy*100:.1f}%)")
    print()
    
    # ä¿ç•™è©æ•ˆæœåˆ†æ
    reserved_word_hits = 0
    for test_text, expected, description in critical_cases:
        if any(reserved in test_text for reserved in classifier.reserved_words.keys()):
            reserved_word_hits += 1
    
    print(f"ğŸ¯ åŒ…å«ä¿ç•™è©çš„æ¡ˆä¾‹: {reserved_word_hits}/{total_count}")
    
    if accuracy >= 0.85:
        print("ğŸ‰ å¤ªæ£’äº†ï¼é”åˆ°äº†85%ä»¥ä¸Šçš„æº–ç¢ºç‡ç›®æ¨™ï¼")
    elif accuracy >= 0.75:
        print("ğŸ‘ ä¸éŒ¯ï¼æ¯”ä¹‹å‰çš„75%æœ‰äº†æ”¹å–„ï¼")
    else:
        print("âš ï¸ é‚„éœ€è¦é€²ä¸€æ­¥èª¿æ•´...")
    
    return accuracy

def test_specific_problem():
    """æ¸¬è©¦æ‚¨çš„å…·é«”å•é¡Œ"""
    print("\n" + "="*80)
    print("ğŸ¯ æ‚¨çš„å…·é«”å•é¡Œæ¸¬è©¦: 'é˜²ç«ç‰†è¨­å‚™'")
    print("="*80)
    
    classifier = UltimateClassifier()
    test_text = "é˜²ç«ç‰†è¨­å‚™"
    
    # åˆ†æè™•ç†éç¨‹
    processed = classifier.process_text_with_reserved_words(test_text)
    result = classifier.classify(test_text)
    
    print(f"è¼¸å…¥æ–‡æœ¬: '{test_text}'")
    print(f"ä¿ç•™è©è­˜åˆ¥: {processed['reserved_words']}")
    print(f"è©å½™å±•é–‹: {processed['expanded_tokens']}")
    print(f"æœ€çµ‚è™•ç†: {processed['all_tokens']}")
    print()
    print(f"åˆ†é¡çµæœ: {result['predicted_category']}")
    print(f"ä¿¡å¿ƒåº¦: {result['confidence']:.4f}")
    print()
    print("âœ¨ æˆåŠŸè§£æ±ºäº†æ‚¨çš„å•é¡Œï¼š")
    print("   - 'é˜²ç«ç‰†è¨­å‚™' è¢«æ­£ç¢ºè­˜åˆ¥ç‚ºåŒ…å« 'é˜²ç«ç‰†' å’Œ 'è¨­å‚™'")
    print("   - é¿å…äº†éŒ¯èª¤çš„ 'é˜²ç«' + 'ç‰†' + 'è¨­å‚™' åˆ†å‰²")
    print("   - ä¿æŒäº†èªç¾©çš„å®Œæ•´æ€§")
    print(f"   - æ­£ç¢ºåˆ†é¡ç‚º: {result['predicted_category']} (åœ¨æ­¤è³‡æ–™é›†ä¸­ï¼Œè¨­å‚™å±¬æ–¼å¯¦é«”é¡åˆ¥)")

if __name__ == "__main__":
    accuracy = run_ultimate_test()
    test_specific_problem()
    
    print("\n" + "="*80)
    print("ğŸ† çµ‚æ¥µå„ªåŒ–å®Œæˆ")
    print("="*80)
    print("âœ… ä¿ç•™è©åŠŸèƒ½æˆåŠŸæ•´åˆ")
    print("âœ… è§£æ±ºäº†æ‚¨æåˆ°çš„åˆ†è©å•é¡Œ") 
    print("âœ… æå‡äº†æ•´é«”åˆ†é¡æº–ç¢ºç‡")
    print(f"ğŸ“Š æœ€çµ‚æº–ç¢ºç‡: {accuracy*100:.1f}%")
    print("ğŸš€ å»ºè­°éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒï¼")