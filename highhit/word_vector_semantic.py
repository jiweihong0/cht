#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºæ–¼è©å‘é‡çš„èªæ„åˆ†æå™¨ - ä¸éœ€è¦é¡å¤–ä¾è³´
"""

import pandas as pd
import jieba
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

class WordVectorSemanticAnalyzer:
    """åŸºæ–¼è©å‘é‡çš„èªæ„åˆ†æå™¨"""
    
    def __init__(self, csv_path='RA_data.csv'):
        """åˆå§‹åŒ–èªæ„åˆ†æå™¨"""
        self.csv_path = csv_path
        self.data = None
        self.word_vectors = {}
        self.category_keywords = {}
        
        # è¼‰å…¥è³‡æ–™
        self.load_data()
        
        # å»ºç«‹èªæ„é—œè¯è©å…¸
        self.build_semantic_dictionary()
        
        # åˆ†æé¡åˆ¥é—œéµè©
        self.analyze_category_keywords()
    
    def load_data(self):
        """è¼‰å…¥CSVè³‡æ–™"""
        try:
            self.data = pd.read_csv(self.csv_path, encoding='utf-8')
            print(f"è¼‰å…¥äº† {len(self.data)} ç­†è³‡æ–™")
        except Exception as e:
            print(f"è¼‰å…¥è³‡æ–™å¤±æ•—: {e}")
            self.data = pd.DataFrame()
    
    def build_semantic_dictionary(self):
        """å»ºç«‹èªæ„é—œè¯è©å…¸"""
        # å®šç¾©èªæ„ç›¸é—œè©ç¾¤
        self.semantic_groups = {
            # è³‡æ–™åº«ç›¸é—œ
            'database': ['è³‡æ–™åº«', 'MySQL', 'Oracle', 'SQL', 'PostgreSQL', 'MongoDB', 'Redis', 'db'],
            
            # ä½œæ¥­ç³»çµ±ç›¸é—œ
            'os': ['ä½œæ¥­ç³»çµ±', 'Windows', 'Linux', 'Unix', 'macOS', 'CentOS', 'Ubuntu', 'ç³»çµ±'],
            
            # äººå“¡ç›¸é—œ
            'personnel': ['äººå“¡', 'ç®¡ç†å“¡', 'å“¡å·¥', 'ä½¿ç”¨è€…', 'æ“ä½œå“¡', 'é–‹ç™¼è€…', 'å·¥ç¨‹å¸«'],
            
            # å¯¦é«”è¨­å‚™ç›¸é—œ
            'hardware': ['ä¼ºæœå™¨', 'è¨­å‚™', 'ä¸»æ©Ÿ', 'é›»è…¦', 'ç­†é›»', 'å°è¡¨æ©Ÿ', 'è·¯ç”±å™¨', 'äº¤æ›å™¨'],
            
            # è»Ÿé«”ç›¸é—œ
            'software': ['è»Ÿé«”', 'æ‡‰ç”¨ç¨‹å¼', 'ç³»çµ±', 'APP', 'ç¨‹å¼', 'å·¥å…·', 'å¹³å°'],
            
            # è³‡æ–™ç›¸é—œ
            'data': ['è³‡æ–™', 'æª”æ¡ˆ', 'æ–‡ä»¶', 'ç´€éŒ„', 'å‚™ä»½', 'æ—¥èªŒ', 'è³‡è¨Š'],
            
            # æœå‹™ç›¸é—œ
            'service': ['æœå‹™', 'é›²ç«¯', 'SaaS', 'PaaS', 'IaaS', 'è¨—ç®¡', 'å¤–åŒ…'],
            
            # ç¶²è·¯ç›¸é—œ
            'network': ['ç¶²è·¯', 'ç¶²éš›ç¶²è·¯', 'WiFi', 'LAN', 'WAN', 'é€£ç·š', 'é€šè¨Š'],
            
            # å®‰å…¨ç›¸é—œ
            'security': ['é˜²ç«ç‰†', 'åŠ å¯†', 'èªè­‰', 'æˆæ¬Š', 'æ†‘è­‰', 'é‡‘é‘°', 'å¯†ç¢¼']
        }
        
        # å»ºç«‹åå‘ç´¢å¼•ï¼šè©å½™ -> èªæ„ç¾¤çµ„
        self.word_to_groups = {}
        for group, words in self.semantic_groups.items():
            for word in words:
                if word not in self.word_to_groups:
                    self.word_to_groups[word] = []
                self.word_to_groups[word].append(group)
    
    def analyze_category_keywords(self):
        """åˆ†ææ¯å€‹é¡åˆ¥çš„é—œéµè©"""
        if self.data.empty:
            return
        
        for category in self.data['è³‡ç”¢é¡åˆ¥'].unique():
            category_assets = self.data[self.data['è³‡ç”¢é¡åˆ¥'] == category]['è³‡ç”¢åç¨±']
            
            # æå–æ‰€æœ‰è©å½™
            all_words = []
            for asset in category_assets:
                words = list(jieba.cut(str(asset)))
                all_words.extend([w for w in words if len(w) > 1])
            
            # è¨ˆç®—è©é »
            word_freq = Counter(all_words)
            
            # å–æœ€å¸¸è¦‹çš„è©ä½œç‚ºè©²é¡åˆ¥çš„ç‰¹å¾µè©
            self.category_keywords[category] = dict(word_freq.most_common(10))
    
    def semantic_similarity(self, word1, word2):
        """è¨ˆç®—å…©å€‹è©çš„èªæ„ç›¸ä¼¼åº¦"""
        # ç›´æ¥åŒ¹é…
        if word1 == word2:
            return 1.0
        
        # æª¢æŸ¥æ˜¯å¦åœ¨åŒä¸€èªæ„ç¾¤çµ„
        groups1 = self.word_to_groups.get(word1, [])
        groups2 = self.word_to_groups.get(word2, [])
        
        common_groups = set(groups1).intersection(set(groups2))
        if common_groups:
            return 0.8  # åŒèªæ„ç¾¤çµ„çš„ç›¸ä¼¼åº¦
        
        # åŒ…å«é—œä¿‚
        if word1 in word2 or word2 in word1:
            return 0.6
        
        # ç·¨è¼¯è·é›¢ç›¸ä¼¼åº¦ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        if len(word1) > 2 and len(word2) > 2:
            if word1[:2] == word2[:2] or word1[-2:] == word2[-2:]:
                return 0.3
        
        return 0.0
    
    def text_semantic_similarity(self, text1, text2):
        """è¨ˆç®—å…©å€‹æ–‡æœ¬çš„èªæ„ç›¸ä¼¼åº¦"""
        words1 = [w for w in jieba.cut(text1) if len(w) > 1]
        words2 = [w for w in jieba.cut(text2) if len(w) > 1]
        
        if not words1 or not words2:
            return 0.0
        
        # è¨ˆç®—è©å°è©çš„æœ€å¤§ç›¸ä¼¼åº¦
        total_similarity = 0
        for w1 in words1:
            max_sim = 0
            for w2 in words2:
                sim = self.semantic_similarity(w1, w2)
                max_sim = max(max_sim, sim)
            total_similarity += max_sim
        
        # æ­£è¦åŒ–
        avg_similarity = total_similarity / len(words1)
        
        # åŠ å…¥é•·åº¦æ‡²ç½°ï¼Œé¿å…çŸ­æ–‡æœ¬ç²å¾—éé«˜åˆ†æ•¸
        length_factor = min(len(words1), len(words2)) / max(len(words1), len(words2))
        
        return avg_similarity * length_factor
    
    def category_relevance_score(self, text, category):
        """è¨ˆç®—æ–‡æœ¬èˆ‡ç‰¹å®šé¡åˆ¥çš„ç›¸é—œæ€§åˆ†æ•¸"""
        if category not in self.category_keywords:
            return 0.0
        
        text_words = [w for w in jieba.cut(text) if len(w) > 1]
        category_words = self.category_keywords[category]
        
        relevance_score = 0
        for word in text_words:
            if word in category_words:
                # æ ¹æ“šè©²è©åœ¨é¡åˆ¥ä¸­çš„é »ç‡çµ¦äºˆæ¬Šé‡
                weight = category_words[word] / sum(category_words.values())
                relevance_score += weight
        
        return relevance_score
    
    def analyze_similarity(self, input_text, top_k=10):
        """åˆ†æèªæ„ç›¸ä¼¼åº¦"""
        if self.data.empty:
            return [], ""
        
        print(f"æ¸¬è©¦æ–‡æœ¬: {input_text}")
        
        # é è™•ç†è¼¸å…¥æ–‡æœ¬
        processed_input = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', input_text)
        print(f"è™•ç†å¾Œæ–‡æœ¬: {processed_input}")
        
        results = []
        
        # è¨ˆç®—èˆ‡æ¯å€‹è³‡ç”¢çš„èªæ„ç›¸ä¼¼åº¦
        for idx, row in self.data.iterrows():
            asset_name = row['è³‡ç”¢åç¨±']
            category = row['è³‡ç”¢é¡åˆ¥']
            
            # èªæ„ç›¸ä¼¼åº¦
            semantic_sim = self.text_semantic_similarity(processed_input, asset_name)
            
            # é¡åˆ¥ç›¸é—œæ€§åˆ†æ•¸
            category_score = self.category_relevance_score(processed_input, category)
            
            # ç¶œåˆåˆ†æ•¸
            combined_score = 0.7 * semantic_sim + 0.3 * category_score
            
            if combined_score > 0:
                results.append({
                    'asset_name': asset_name,
                    'category': category,
                    'similarity': combined_score,
                    'semantic_similarity': semantic_sim,
                    'category_relevance': category_score
                })
        
        # æ’åºä¸¦è¿”å›å‰ k å€‹çµæœ
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        print("=" * 60)
        return results[:top_k], processed_input
    
    def analyze_with_explanation(self, input_text, top_k=5):
        """å¸¶è§£é‡‹çš„ç›¸ä¼¼åº¦åˆ†æ"""
        print(f"\nğŸ” èªæ„åˆ†æ: {input_text}")
        print("=" * 60)
        
        results, processed = self.analyze_similarity(input_text, top_k)
        
        if results:
            print(f"æ‰¾åˆ° {len(results)} å€‹ç›¸ä¼¼é …ç›®:\n")
            
            for i, result in enumerate(results, 1):
                print(f"{i}. {result['asset_name']} (é¡åˆ¥: {result['category']})")
                print(f"   ç¶œåˆåˆ†æ•¸: {result['similarity']:.4f}")
                print(f"   èªæ„ç›¸ä¼¼åº¦: {result['semantic_similarity']:.4f}")
                print(f"   é¡åˆ¥ç›¸é—œæ€§: {result['category_relevance']:.4f}")
                
                # é¡¯ç¤ºåŒ¹é…çš„é—œéµè©
                input_words = set(jieba.cut(input_text))
                asset_words = set(jieba.cut(result['asset_name']))
                common_words = input_words.intersection(asset_words)
                if common_words:
                    print(f"   å…±åŒè©å½™: {', '.join(common_words)}")
                
                print()
        else:
            print("æœªæ‰¾åˆ°ç›¸ä¼¼é …ç›®")
        
        return results, processed

def test_word_vector_semantic():
    """æ¸¬è©¦è©å‘é‡èªæ„åˆ†æ"""
    print("=" * 80)
    print("ğŸ§ª è©å‘é‡èªæ„åˆ†ææ¸¬è©¦")
    print("=" * 80)
    
    analyzer = WordVectorSemanticAnalyzer('RA_data.csv')
    
    # é¡¯ç¤ºé¡åˆ¥é—œéµè©
    print("ğŸ“Š å„é¡åˆ¥ç‰¹å¾µè©:")
    for category, keywords in analyzer.category_keywords.items():
        top_words = list(keywords.keys())[:5]
        print(f"  {category}: {', '.join(top_words)}")
    
    print("\n" + "=" * 80)
    
    test_cases = [
        "MySQL è³‡æ–™åº«",
        "Windows ä½œæ¥­ç³»çµ±",
        "ç³»çµ±ç®¡ç†å“¡",
        "å‚™ä»½æª”æ¡ˆ",
        "é˜²ç«ç‰†è¨­å‚™",
        "é›²ç«¯ä¼ºæœå™¨",
        "ç¶²è·¯è·¯ç”±å™¨"
    ]
    
    for test_text in test_cases:
        analyzer.analyze_with_explanation(test_text, top_k=3)
        print("-" * 80)

if __name__ == "__main__":
    test_word_vector_semantic()