#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆè³‡ç”¢åˆ†é¡å™¨ v3.0 - åŒ…å«ä¿ç•™è©åŠŸèƒ½
é‡å°ä¿ç•™è©è™•ç†å’Œæ›´ç²¾ç¢ºçš„æ–‡æœ¬åˆ†å‰²é€²è¡Œå„ªåŒ–
"""

import pandas as pd
import numpy as np
import re
from collections import defaultdict, Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
import jieba.posseg as pseg

class ReservedWordProcessor:
    """ä¿ç•™è©è™•ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¿ç•™è©è™•ç†å™¨"""
        # å®šç¾©ä¿ç•™è©å…¸ - é€™äº›è©æ‡‰è©²ä½œç‚ºå®Œæ•´å–®ä½ä¿ç•™
        self.reserved_words = {
            # æŠ€è¡“è©å½™
            'é˜²ç«ç‰†': ['é˜²ç«ç‰†'],
            'è³‡æ–™åº«': ['è³‡æ–™åº«'],
            'ä½œæ¥­ç³»çµ±': ['ä½œæ¥­ç³»çµ±'],
            'ç®¡ç†ç³»çµ±': ['ç®¡ç†ç³»çµ±'],
            'å„²å­˜åª’é«”': ['å„²å­˜åª’é«”'],
            'æ‡‰ç”¨ç¨‹å¼': ['æ‡‰ç”¨ç¨‹å¼'],
            'ç¶²è·¯è¨­å‚™': ['ç¶²è·¯è¨­å‚™'],
            'ä¼ºæœå™¨': ['ä¼ºæœå™¨'],
            'è™›æ“¬æ©Ÿ': ['è™›æ“¬æ©Ÿ'],
            'å®¹å™¨åŒ–': ['å®¹å™¨åŒ–'],
            
            # è¤‡åˆæŠ€è¡“è©
            'è³‡æ–™åº«ç®¡ç†ç³»çµ±': ['è³‡æ–™åº«ç®¡ç†ç³»çµ±', 'è³‡æ–™åº«', 'ç®¡ç†ç³»çµ±'],
            'ç¶²è·¯é˜²ç«ç‰†': ['ç¶²è·¯é˜²ç«ç‰†', 'ç¶²è·¯', 'é˜²ç«ç‰†'],
            'å¯æ”œå¼å„²å­˜åª’é«”': ['å¯æ”œå¼å„²å­˜åª’é«”', 'å¯æ”œå¼', 'å„²å­˜åª’é«”'],
            'å‚™ä»½ç®¡ç†ç³»çµ±': ['å‚™ä»½ç®¡ç†ç³»çµ±', 'å‚™ä»½', 'ç®¡ç†ç³»çµ±'],
            'ç›£æ§ç®¡ç†ç³»çµ±': ['ç›£æ§ç®¡ç†ç³»çµ±', 'ç›£æ§', 'ç®¡ç†ç³»çµ±'],
            
            # çµ„ç¹”è©å½™
            'å…§éƒ¨äººå“¡': ['å…§éƒ¨äººå“¡', 'å…§éƒ¨', 'äººå“¡'],
            'å¤–éƒ¨äººå“¡': ['å¤–éƒ¨äººå“¡', 'å¤–éƒ¨', 'äººå“¡'],
            'æ‰¿è¾¦äºº': ['æ‰¿è¾¦äºº'],
            'ç®¡ç†å“¡': ['ç®¡ç†å“¡'],
            'ä½¿ç”¨è€…': ['ä½¿ç”¨è€…'],
            
            # æ–‡ä»¶é¡å‹
            'ä½œæ¥­æ–‡ä»¶': ['ä½œæ¥­æ–‡ä»¶', 'ä½œæ¥­', 'æ–‡ä»¶'],
            'é›»å­ç´€éŒ„': ['é›»å­ç´€éŒ„', 'é›»å­', 'ç´€éŒ„'],
            'ç¨‹åºæ–‡ä»¶': ['ç¨‹åºæ–‡ä»¶', 'ç¨‹åº', 'æ–‡ä»¶'],
            'æŠ€è¡“æ–‡ä»¶': ['æŠ€è¡“æ–‡ä»¶', 'æŠ€è¡“', 'æ–‡ä»¶'],
            'æ“ä½œæ‰‹å†Š': ['æ“ä½œæ‰‹å†Š', 'æ“ä½œ', 'æ‰‹å†Š'],
            
            # æœå‹™é¡å‹
            'ç¶²è·¯æœå‹™': ['ç¶²è·¯æœå‹™', 'ç¶²è·¯', 'æœå‹™'],
            'æ‡‰ç”¨æœå‹™': ['æ‡‰ç”¨æœå‹™', 'æ‡‰ç”¨', 'æœå‹™'],
            'è³‡æ–™æœå‹™': ['è³‡æ–™æœå‹™', 'è³‡æ–™', 'æœå‹™'],
            'é›²ç«¯æœå‹™': ['é›²ç«¯æœå‹™', 'é›²ç«¯', 'æœå‹™'],
            
            # è¨­å‚™é¡å‹
            'é˜²ç«ç‰†è¨­å‚™': ['é˜²ç«ç‰†è¨­å‚™', 'é˜²ç«ç‰†', 'è¨­å‚™'],
            'ç¶²è·¯è¨­å‚™': ['ç¶²è·¯è¨­å‚™', 'ç¶²è·¯', 'è¨­å‚™'],
            'å„²å­˜è¨­å‚™': ['å„²å­˜è¨­å‚™', 'å„²å­˜', 'è¨­å‚™'],
            'å®‰å…¨è¨­å‚™': ['å®‰å…¨è¨­å‚™', 'å®‰å…¨', 'è¨­å‚™'],
            'ç›£æ§è¨­å‚™': ['ç›£æ§è¨­å‚™', 'ç›£æ§', 'è¨­å‚™'],
            
            # å¸¸è¦‹å“ç‰Œå’ŒæŠ€è¡“
            'MySQL': ['MySQL'],
            'Oracle': ['Oracle'],
            'SQL Server': ['SQL Server', 'SQL', 'Server'],
            'Windows': ['Windows'],
            'Linux': ['Linux'],
            'Microsoft': ['Microsoft'],
            'VMware': ['VMware'],
            'Docker': ['Docker'],
            
            # è¾¦å…¬ç›¸é—œ
            'è¾¦å…¬å®¤': ['è¾¦å…¬å®¤'],
            'æœƒè­°å®¤': ['æœƒè­°å®¤'],
            'æ©Ÿæˆ¿': ['æ©Ÿæˆ¿'],
            'è³‡æ–™ä¸­å¿ƒ': ['è³‡æ–™ä¸­å¿ƒ', 'è³‡æ–™', 'ä¸­å¿ƒ'],
            
            # å…¶ä»–é‡è¦è©å½™
            'API': ['API'],
            'SOP': ['SOP'],
            'ERP': ['ERP'],
            'CRM': ['CRM']
        }
        
        # å»ºç«‹åå‘ç´¢å¼• - ç”¨æ–¼å¿«é€ŸæŸ¥æ‰¾
        self.word_to_reserved = {}
        for reserved_phrase, components in self.reserved_words.items():
            for component in components:
                if component not in self.word_to_reserved:
                    self.word_to_reserved[component] = []
                self.word_to_reserved[component].append(reserved_phrase)
        
        # è¨»å†Šä¿ç•™è©åˆ° jieba
        self._register_reserved_words()
    
    def _register_reserved_words(self):
        """è¨»å†Šä¿ç•™è©åˆ° jieba åˆ†è©å™¨"""
        for reserved_phrase in self.reserved_words.keys():
            jieba.add_word(reserved_phrase, freq=10000)  # é«˜é »ç‡ç¢ºä¿è¢«è­˜åˆ¥
    
    def extract_reserved_words(self, text):
        """
        å¾æ–‡æœ¬ä¸­æå–ä¿ç•™è©
        Args:
            text: è¼¸å…¥æ–‡æœ¬
        Returns:
            dict: åŒ…å«æ‰¾åˆ°çš„ä¿ç•™è©å’Œè™•ç†å¾Œæ–‡æœ¬çš„å­—å…¸
        """
        found_reserved = []
        remaining_text = text
        
        # æŒ‰é•·åº¦æ’åºï¼Œå„ªå…ˆåŒ¹é…è¼ƒé•·çš„ä¿ç•™è©
        sorted_reserved = sorted(self.reserved_words.keys(), key=len, reverse=True)
        
        for reserved_phrase in sorted_reserved:
            if reserved_phrase in text:
                found_reserved.append(reserved_phrase)
                # ç”¨ä½”ä½ç¬¦æ›¿æ›ï¼Œé¿å…é‡è¤‡åŒ¹é…
                remaining_text = remaining_text.replace(reserved_phrase, f' [RESERVED_{len(found_reserved)}] ')
        
        return {
            'found_reserved': found_reserved,
            'remaining_text': remaining_text.strip(),
            'original_text': text
        }
    
    def process_with_reserved_words(self, text):
        """
        ä½¿ç”¨ä¿ç•™è©è™•ç†æ–‡æœ¬
        Args:
            text: è¼¸å…¥æ–‡æœ¬
        Returns:
            dict: è™•ç†çµæœ
        """
        # æå–ä¿ç•™è©
        reserved_result = self.extract_reserved_words(text)
        
        # å°å‰©é¤˜æ–‡æœ¬é€²è¡Œæ­£å¸¸åˆ†è©
        remaining_words = []
        if reserved_result['remaining_text']:
            # ç§»é™¤ä½”ä½ç¬¦ä¸¦åˆ†è©
            clean_remaining = re.sub(r'\[RESERVED_\d+\]', '', reserved_result['remaining_text'])
            if clean_remaining.strip():
                remaining_words = [w for w in jieba.cut(clean_remaining.strip()) if len(w.strip()) > 0]
        
        # çµ„åˆçµæœ
        all_tokens = reserved_result['found_reserved'] + remaining_words
        
        return {
            'reserved_words': reserved_result['found_reserved'],
            'regular_words': remaining_words,
            'all_tokens': all_tokens,
            'original_text': text
        }

class EnhancedClassifierV3:
    """å¢å¼·ç‰ˆåˆ†é¡å™¨ v3.0 - åŒ…å«ä¿ç•™è©åŠŸèƒ½"""
    
    def __init__(self, data_path='RA_data.csv'):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆåˆ†é¡å™¨ v3.0
        Args:
            data_path: è³‡æ–™æª”æ¡ˆè·¯å¾‘
        """
        self.data_path = data_path
        self.data = None
        self.reserved_processor = ReservedWordProcessor()
        self.category_keywords = {}
        self.category_patterns = {}
        self.exclusion_rules = {}
        self.vectorizer = None
        self.category_vectors = {}
        
        # è¼‰å…¥æ•¸æ“šä¸¦åˆå§‹åŒ–
        self.load_data()
        self.build_enhanced_features()
        self.create_category_rules()
    
    def load_data(self):
        """è¼‰å…¥è³‡æ–™"""
        try:
            self.data = pd.read_csv(self.data_path, encoding='utf-8')
            print(f"âœ… è¼‰å…¥è³‡æ–™ï¼š{len(self.data)} ç­†è¨˜éŒ„")
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—ï¼š{e}")
            self.data = pd.DataFrame()
    
    def preprocess_text(self, text):
        """
        å¢å¼·ç‰ˆæ–‡æœ¬é è™•ç†ï¼ˆåŒ…å«ä¿ç•™è©è™•ç†ï¼‰
        Args:
            text: è¼¸å…¥æ–‡æœ¬
        Returns:
            dict: åŒ…å«å¤šç¨®è™•ç†çµæœçš„å­—å…¸
        """
        if not isinstance(text, str):
            text = str(text)
        
        # åŸå§‹æ–‡æœ¬
        original = text.strip()
        
        # åŸºæœ¬æ¸…ç†
        cleaned = re.sub(r'\s+', ' ', text.strip())
        
        # ä¿ç•™è©è™•ç†
        reserved_result = self.reserved_processor.process_with_reserved_words(cleaned)
        
        # ç§»é™¤æ‹¬è™Ÿå…§å®¹
        no_brackets = re.sub(r'\([^)]*\)', '', cleaned).strip()
        
        # æå–æ‹¬è™Ÿå…§å®¹
        bracket_content = ""
        bracket_match = re.search(r'\(([^)]*)\)', cleaned)
        if bracket_match:
            bracket_content = bracket_match.group(1).strip()
        
        # å¤§å°å¯«è®ŠåŒ–
        lower_case = cleaned.lower()
        
        # ç§»é™¤ç©ºæ ¼
        no_spaces = re.sub(r'\s+', '', cleaned)
        
        # å‚³çµ±åˆ†è©ï¼ˆä½œç‚ºå‚™é¸ï¼‰
        traditional_words = list(jieba.cut(cleaned))
        traditional_words_filtered = [w for w in traditional_words if len(w.strip()) > 1 and w.strip() not in ['çš„', 'èˆ‡', 'åŠ', 'å’Œ', 'æˆ–']]
        
        # è©æ€§æ¨™è¨»
        pos_tags = [(word, flag) for word, flag in pseg.cut(cleaned)]
        
        return {
            'original': original,
            'cleaned': cleaned,
            'no_brackets': no_brackets,
            'bracket_content': bracket_content,
            'lower_case': lower_case,
            'no_spaces': no_spaces,
            'reserved_words': reserved_result['reserved_words'],
            'regular_words': reserved_result['regular_words'],
            'all_tokens': reserved_result['all_tokens'],
            'traditional_words': traditional_words,
            'traditional_words_filtered': traditional_words_filtered,
            'pos_tags': pos_tags
        }
    
    def build_enhanced_features(self):
        """å»ºç«‹å¢å¼·ç‰¹å¾µ"""
        if self.data.empty:
            return
        
        # ç‚ºæ¯å€‹é¡åˆ¥å»ºç«‹é—œéµè©é›†åˆ
        category_texts = defaultdict(list)
        
        for _, row in self.data.iterrows():
            category = row['è³‡ç”¢é¡åˆ¥']
            asset_name = row['è³‡ç”¢åç¨±']
            
            # é è™•ç†è³‡ç”¢åç¨±
            processed = self.preprocess_text(asset_name)
            
            # æ”¶é›†è©²é¡åˆ¥çš„æ‰€æœ‰æ–‡æœ¬è®ŠåŒ–
            category_texts[category].extend([
                processed['cleaned'],
                processed['no_brackets'],
                processed['bracket_content'],
                ' '.join(processed['all_tokens']),  # ä½¿ç”¨ä¿ç•™è©è™•ç†å¾Œçš„çµæœ
                ' '.join(processed['traditional_words_filtered'])  # å‚³çµ±åˆ†è©ä½œç‚ºå‚™é¸
            ])
        
        # å»ºç«‹ TF-IDF å‘é‡åŒ–å™¨
        all_texts = []
        category_labels = []
        
        for category, texts in category_texts.items():
            for text in texts:
                if text.strip():
                    all_texts.append(text)
                    category_labels.append(category)
        
        # ä½¿ç”¨å­—ç¬¦ç´šåˆ¥å’Œè©ç´šåˆ¥çš„æ··åˆ n-gram
        self.vectorizer = TfidfVectorizer(
            analyzer='char_wb',
            ngram_range=(2, 4),
            max_features=8000,  # å¢åŠ ç‰¹å¾µæ•¸é‡
            lowercase=True,
            token_pattern=r'(?u)\b\w+\b'  # åŒ…å«ä¸­æ–‡å­—ç¬¦
        )
        
        if all_texts:
            vectors = self.vectorizer.fit_transform(all_texts)
            
            # ç‚ºæ¯å€‹é¡åˆ¥è¨ˆç®—å¹³å‡å‘é‡
            for category in category_texts.keys():
                category_indices = [i for i, label in enumerate(category_labels) if label == category]
                if category_indices:
                    category_vector = vectors[category_indices].mean(axis=0)
                    self.category_vectors[category] = category_vector
    
    def create_category_rules(self):
        """å‰µå»ºå¢å¼·çš„é¡åˆ¥è¦å‰‡"""
        # å¼·åŒ–çš„é—œéµè©è¦å‰‡ï¼ˆåŒ…å«ä¿ç•™è©ï¼‰
        self.category_keywords = {
            'è»Ÿé«”': {
                'strong': ['ç³»çµ±', 'è»Ÿé«”', 'æ‡‰ç”¨ç¨‹å¼', 'è³‡æ–™åº«', 'ç¨‹å¼', 'èªè¨€', 'å¹³å°', 'æ¡†æ¶', 
                          'è³‡æ–™åº«ç®¡ç†ç³»çµ±', 'MySQL', 'Oracle', 'SQL Server', 'Windows', 'Linux'],
                'medium': ['server', 'sql', 'unix', 'java', 'python', '.net', 'asp', 'API', 'ERP', 'CRM'],
                'weak': ['ç®¡ç†', 'é–‹ç™¼', 'æœå‹™å™¨', 'æ‡‰ç”¨']
            },
            'ç¡¬é«”': {
                'strong': ['ç¡¬é«”', 'è¨­å‚™', 'ä¼ºæœå™¨', 'ä¸»æ©Ÿ', 'é›»è…¦', 'ç¶²è·¯è¨­å‚™', 'å„²å­˜è¨­å‚™', 
                          'é˜²ç«ç‰†è¨­å‚™', 'ç›£æ§è¨­å‚™', 'å®‰å…¨è¨­å‚™'],
                'medium': ['server', 'äº¤æ›å™¨', 'è·¯ç”±å™¨', 'é˜²ç«ç‰†', 'å°è¡¨æ©Ÿ', 'å„²å­˜'],
                'weak': ['æ©Ÿå™¨', 'è¨­æ–½', 'çµ‚ç«¯', 'è£ç½®']
            },
            'å¯¦é«”': {
                'strong': ['å¯¦é«”', 'ç’°å¢ƒ', 'è¨­æ–½', 'å ´æ‰€', 'ç©ºé–“', 'æ©Ÿæˆ¿', 'è¾¦å…¬å®¤', 'æœƒè­°å®¤', 'è³‡æ–™ä¸­å¿ƒ'],
                'medium': ['å»ºç¯‰', 'å ´åœ°', 'ä½ç½®', 'å€åŸŸ', 'å¯æ”œå¼å„²å­˜åª’é«”'],
                'weak': ['åœ°é»', 'è™•æ‰€', 'åª’é«”']
            },
            'è³‡æ–™': {
                'strong': ['è³‡æ–™', 'æ–‡ä»¶', 'æª”æ¡ˆ', 'ç´€éŒ„', 'åˆç´„', 'æ–‡æª”', 'ä½œæ¥­æ–‡ä»¶', 'é›»å­ç´€éŒ„', 
                          'ç¨‹åºæ–‡ä»¶', 'æŠ€è¡“æ–‡ä»¶', 'æ“ä½œæ‰‹å†Š'],
                'medium': ['ä½œæ¥­', 'ç¨‹åº', 'SOP', 'å‚™ä»½', 'æ—¥èªŒ', 'åŸå§‹ç¢¼', 'æ‰‹å†Š'],
                'weak': ['ç´€éŒ„', 'è³‡è¨Š', 'å…§å®¹', 'å ±å‘Š']
            },
            'äººå“¡': {
                'strong': ['äººå“¡', 'å“¡å·¥', 'è·å“¡', 'ä½¿ç”¨è€…', 'ç”¨æˆ¶', 'ç®¡ç†å“¡', 'å…§éƒ¨äººå“¡', 'å¤–éƒ¨äººå“¡', 'æ‰¿è¾¦äºº'],
                'medium': ['å…§éƒ¨', 'å¤–éƒ¨', 'å®¢æˆ¶', 'å» å•†', 'å§”å¤–'],
                'weak': ['äºº', 'è€…', 'å·¥ä½œäººå“¡']
            },
            'æœå‹™': {
                'strong': ['æœå‹™', 'æ‡‰ç”¨æœå‹™', 'ç³»çµ±æœå‹™', 'ç¶²è·¯æœå‹™', 'é›²ç«¯æœå‹™', 'è³‡æ–™æœå‹™'],
                'medium': ['api', 'web', 'ç¶²ç«™', 'å…¥å£ç¶²ç«™', 'æ‡‰ç”¨'],
                'weak': ['åŠŸèƒ½', 'æ”¯æ´', 'å¹³å°']
            }
        }
        
        # æ’é™¤è¦å‰‡ - é¿å…éŒ¯èª¤åˆ†é¡
        self.exclusion_rules = {
            'äººå“¡': {
                'exclude_if_contains': ['æ–‡ä»¶', 'æª”æ¡ˆ', 'è³‡æ–™', 'ç¨‹åº', 'ç³»çµ±', 'è¨­å‚™', 'æœå‹™', 'è³‡æ–™åº«'],
                'exclude_reserved_words': ['ä½œæ¥­æ–‡ä»¶', 'é›»å­ç´€éŒ„', 'ç¨‹åºæ–‡ä»¶', 'æŠ€è¡“æ–‡ä»¶']
            },
            'è³‡æ–™': {
                'include_reserved_words': ['ä½œæ¥­æ–‡ä»¶', 'é›»å­ç´€éŒ„', 'ç¨‹åºæ–‡ä»¶', 'æŠ€è¡“æ–‡ä»¶', 'æ“ä½œæ‰‹å†Š'],
                'include_if_contains': ['æ–‡ä»¶', 'æª”æ¡ˆ', 'ç´€éŒ„', 'åˆç´„', 'ä½œæ¥­', 'SOP']
            },
            'è»Ÿé«”': {
                'include_reserved_words': ['è³‡æ–™åº«ç®¡ç†ç³»çµ±', 'MySQL', 'Oracle', 'SQL Server', 'Windows', 'Linux'],
                'exclude_if_reserved_and_contains': [('é˜²ç«ç‰†', 'è¨­å‚™')]  # é˜²ç«ç‰†+è¨­å‚™ -> ç¡¬é«”
            },
            'ç¡¬é«”': {
                'include_reserved_words': ['é˜²ç«ç‰†è¨­å‚™', 'ç¶²è·¯è¨­å‚™', 'å„²å­˜è¨­å‚™', 'ç›£æ§è¨­å‚™', 'å®‰å…¨è¨­å‚™'],
                'include_if_reserved_and_contains': [('é˜²ç«ç‰†', 'è¨­å‚™')]
            }
        }
        
        # æ­£å‰‡è¡¨é”å¼æ¨¡å¼ï¼ˆæ›´æ–°ä»¥åŒ…å«ä¿ç•™è©ï¼‰
        self.category_patterns = {
            'è»Ÿé«”': [
                r'.*ç³»çµ±$', r'.*è»Ÿé«”$', r'.*ç¨‹å¼.*', r'.*è³‡æ–™åº«.*',
                r'.*(windows|linux|unix|sql|mysql|oracle).*',
                r'è³‡æ–™åº«ç®¡ç†ç³»çµ±', r'ç®¡ç†ç³»çµ±'
            ],
            'ç¡¬é«”': [
                r'.*è¨­å‚™$', r'.*ä¸»æ©Ÿ$', r'.*ä¼ºæœå™¨.*', r'.*é›»è…¦.*',
                r'.*(server|äº¤æ›å™¨|è·¯ç”±å™¨).*', r'é˜²ç«ç‰†è¨­å‚™', r'ç¶²è·¯è¨­å‚™'
            ],
            'è³‡æ–™': [
                r'.*æ–‡ä»¶.*', r'.*æª”æ¡ˆ.*', r'.*ç´€éŒ„.*', r'.*åˆç´„.*',
                r'.*(sop|å‚™ä»½|æ—¥èªŒ|åŸå§‹ç¢¼).*', r'ä½œæ¥­æ–‡ä»¶', r'é›»å­ç´€éŒ„'
            ],
            'äººå“¡': [
                r'.*äººå“¡$', r'.*å“¡å·¥.*', r'.*ä½¿ç”¨è€….*', r'.*ç®¡ç†å“¡.*',
                r'å…§éƒ¨äººå“¡', r'å¤–éƒ¨äººå“¡', r'æ‰¿è¾¦äºº'
            ],
            'æœå‹™': [
                r'.*æœå‹™.*', r'.*æ‡‰ç”¨.*', r'.*(api|web|ç¶²ç«™).*',
                r'ç¶²è·¯æœå‹™', r'é›²ç«¯æœå‹™', r'æ‡‰ç”¨æœå‹™'
            ]
        }
    
    def calculate_reserved_word_score(self, text_variants, category):
        """
        è¨ˆç®—ä¿ç•™è©åŒ¹é…åˆ†æ•¸
        Args:
            text_variants: æ–‡æœ¬çš„å„ç¨®è®ŠåŒ–å½¢å¼
            category: ç›®æ¨™é¡åˆ¥
        Returns:
            float: ä¿ç•™è©åŒ¹é…åˆ†æ•¸
        """
        if category not in self.category_keywords:
            return 0.0
        
        reserved_words = text_variants.get('reserved_words', [])
        if not reserved_words:
            return 0.0
        
        keywords = self.category_keywords[category]
        score = 0.0
        total_matches = 0
        
        # æª¢æŸ¥ä¿ç•™è©æ˜¯å¦åœ¨é—œéµè©åˆ—è¡¨ä¸­
        for reserved_word in reserved_words:
            found_in_strong = any(keyword in reserved_word or reserved_word in keyword 
                                for keyword in keywords.get('strong', []))
            found_in_medium = any(keyword in reserved_word or reserved_word in keyword 
                                for keyword in keywords.get('medium', []))
            found_in_weak = any(keyword in reserved_word or reserved_word in keyword 
                              for keyword in keywords.get('weak', []))
            
            if found_in_strong:
                score += 4.0  # ä¿ç•™è©åŒ¹é…çµ¦äºˆæ›´é«˜æ¬Šé‡
                total_matches += 1
            elif found_in_medium:
                score += 3.0
                total_matches += 1
            elif found_in_weak:
                score += 2.0
                total_matches += 1
        
        # æ­£è¦åŒ–åˆ†æ•¸
        if total_matches > 0:
            return min(score / len(reserved_words), 1.0)  # é™åˆ¶æœ€é«˜åˆ†æ•¸ç‚º1.0
        
        return 0.0
    
    def calculate_keyword_score(self, text_variants, category):
        """
        è¨ˆç®—é—œéµè©åŒ¹é…åˆ†æ•¸ï¼ˆå¢å¼·ç‰ˆï¼‰
        Args:
            text_variants: æ–‡æœ¬çš„å„ç¨®è®ŠåŒ–å½¢å¼
            category: ç›®æ¨™é¡åˆ¥
        Returns:
            float: é—œéµè©åŒ¹é…åˆ†æ•¸
        """
        if category not in self.category_keywords:
            return 0.0
        
        keywords = self.category_keywords[category]
        score = 0.0
        
        # æª¢æŸ¥æ‰€æœ‰æ–‡æœ¬è®ŠåŒ–ï¼ˆåŒ…æ‹¬ä¿ç•™è©è™•ç†å¾Œçš„çµæœï¼‰
        all_text = ' '.join([
            text_variants.get('cleaned', ''),
            text_variants.get('no_brackets', ''),
            text_variants.get('bracket_content', ''),
            ' '.join(text_variants.get('all_tokens', [])),
            ' '.join(text_variants.get('reserved_words', []))
        ]).lower()
        
        # å¼·é—œéµè© (æ¬Šé‡ 3.0)
        for keyword in keywords.get('strong', []):
            if keyword.lower() in all_text:
                score += 3.0
        
        # ä¸­é—œéµè© (æ¬Šé‡ 2.0)
        for keyword in keywords.get('medium', []):
            if keyword.lower() in all_text:
                score += 2.0
        
        # å¼±é—œéµè© (æ¬Šé‡ 1.0)
        for keyword in keywords.get('weak', []):
            if keyword.lower() in all_text:
                score += 1.0
        
        # æ­£è¦åŒ–åˆ†æ•¸
        max_possible_score = (
            len(keywords.get('strong', [])) * 3.0 +
            len(keywords.get('medium', [])) * 2.0 +
            len(keywords.get('weak', [])) * 1.0
        )
        
        return score / max_possible_score if max_possible_score > 0 else 0.0
    
    def calculate_pattern_score(self, text_variants, category):
        """
        è¨ˆç®—æ¨¡å¼åŒ¹é…åˆ†æ•¸ï¼ˆå¢å¼·ç‰ˆï¼‰
        Args:
            text_variants: æ–‡æœ¬çš„å„ç¨®è®ŠåŒ–å½¢å¼
            category: ç›®æ¨™é¡åˆ¥
        Returns:
            float: æ¨¡å¼åŒ¹é…åˆ†æ•¸
        """
        if category not in self.category_patterns:
            return 0.0
        
        patterns = self.category_patterns[category]
        
        # æª¢æŸ¥æ‰€æœ‰æ–‡æœ¬è®ŠåŒ–ï¼ˆåŒ…æ‹¬ä¿ç•™è©ï¼‰
        test_texts = [
            text_variants.get('cleaned', ''),
            text_variants.get('no_brackets', ''),
            text_variants.get('bracket_content', ''),
            text_variants.get('lower_case', ''),
            ' '.join(text_variants.get('reserved_words', [])),
            ' '.join(text_variants.get('all_tokens', []))
        ]
        
        match_count = 0
        for pattern in patterns:
            for text in test_texts:
                if text and re.search(pattern, text, re.IGNORECASE):
                    match_count += 1
                    break
        
        return match_count / len(patterns) if patterns else 0.0
    
    def calculate_similarity_score(self, text_variants, category):
        """
        è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸ï¼ˆå¢å¼·ç‰ˆï¼‰
        Args:
            text_variants: æ–‡æœ¬çš„å„ç¨®è®ŠåŒ–å½¢å¼  
            category: ç›®æ¨™é¡åˆ¥
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•¸
        """
        if not self.vectorizer or category not in self.category_vectors:
            return 0.0
        
        # çµ„åˆæ‰€æœ‰æ–‡æœ¬è®ŠåŒ–ï¼ˆåŒ…æ‹¬ä¿ç•™è©ï¼‰
        combined_text = ' '.join([
            text_variants.get('cleaned', ''),
            text_variants.get('no_brackets', ''),
            text_variants.get('bracket_content', ''),
            ' '.join(text_variants.get('all_tokens', []))
        ]).strip()
        
        if not combined_text:
            return 0.0
        
        try:
            # å‘é‡åŒ–è¼¸å…¥æ–‡æœ¬
            input_vector = self.vectorizer.transform([combined_text])
            
            # è¨ˆç®—èˆ‡é¡åˆ¥å‘é‡çš„ç›¸ä¼¼åº¦
            similarity = cosine_similarity(input_vector, self.category_vectors[category])[0, 0]
            return float(similarity)
        except:
            return 0.0
    
    def apply_exclusion_rules(self, text_variants, category, base_score):
        """
        æ‡‰ç”¨æ’é™¤è¦å‰‡ï¼ˆå¢å¼·ç‰ˆ - åŒ…å«ä¿ç•™è©è¦å‰‡ï¼‰
        Args:
            text_variants: æ–‡æœ¬çš„å„ç¨®è®ŠåŒ–å½¢å¼
            category: ç›®æ¨™é¡åˆ¥
            base_score: åŸºç¤åˆ†æ•¸
        Returns:
            float: èª¿æ•´å¾Œçš„åˆ†æ•¸
        """
        if category not in self.exclusion_rules:
            return base_score
        
        rules = self.exclusion_rules[category]
        reserved_words = text_variants.get('reserved_words', [])
        
        combined_text = ' '.join([
            text_variants.get('cleaned', ''),
            text_variants.get('no_brackets', ''),
            text_variants.get('bracket_content', '')
        ]).lower()
        
        # ä¿ç•™è©åŒ…å«è¦å‰‡
        if 'include_reserved_words' in rules:
            for include_word in rules['include_reserved_words']:
                if include_word in reserved_words:
                    return base_score * 2.0  # å¤§å¹…å¢å¼·åˆ†æ•¸
        
        # ä¿ç•™è©æ’é™¤è¦å‰‡
        if 'exclude_reserved_words' in rules:
            for exclude_word in rules['exclude_reserved_words']:
                if exclude_word in reserved_words:
                    return base_score * 0.2  # å¤§å¹…é™ä½åˆ†æ•¸
        
        # ä¿ç•™è©+æ¢ä»¶è¦å‰‡
        if 'include_if_reserved_and_contains' in rules:
            for reserved_word, condition in rules['include_if_reserved_and_contains']:
                if reserved_word in reserved_words and condition in combined_text:
                    return base_score * 2.0
        
        if 'exclude_if_reserved_and_contains' in rules:
            for reserved_word, condition in rules['exclude_if_reserved_and_contains']:
                if reserved_word in reserved_words and condition in combined_text:
                    return base_score * 0.2
        
        # å‚³çµ±æ’é™¤è¦å‰‡
        if 'exclude_if_contains' in rules:
            for exclude_word in rules['exclude_if_contains']:
                if exclude_word in combined_text:
                    return base_score * 0.3
        
        # å‚³çµ±åŒ…å«è¦å‰‡
        if 'include_if_contains' in rules:
            for include_word in rules['include_if_contains']:
                if include_word in combined_text:
                    return base_score * 1.5
        
        return base_score
    
    def classify_text(self, input_text, method='enhanced_v3'):
        """
        å¢å¼·ç‰ˆæ–‡æœ¬åˆ†é¡ v3.0
        Args:
            input_text: è¼¸å…¥æ–‡æœ¬
            method: åˆ†é¡æ–¹æ³•
        Returns:
            dict: åˆ†é¡çµæœ
        """
        if self.data.empty:
            return {'error': 'æ²’æœ‰å¯ç”¨çš„è¨“ç·´è³‡æ–™'}
        
        # é è™•ç†è¼¸å…¥æ–‡æœ¬
        text_variants = self.preprocess_text(input_text)
        
        # ç²å–æ‰€æœ‰é¡åˆ¥
        categories = self.data['è³‡ç”¢é¡åˆ¥'].unique()
        category_scores = {}
        
        for category in categories:
            # è¨ˆç®—å¤šç¨®åˆ†æ•¸
            reserved_score = self.calculate_reserved_word_score(text_variants, category)
            keyword_score = self.calculate_keyword_score(text_variants, category)
            pattern_score = self.calculate_pattern_score(text_variants, category)
            similarity_score = self.calculate_similarity_score(text_variants, category)
            
            # åŠ æ¬Šçµ„åˆåˆ†æ•¸ï¼ˆä¿ç•™è©çµ¦äºˆæœ€é«˜æ¬Šé‡ï¼‰
            combined_score = (
                reserved_score * 0.4 +      # ä¿ç•™è©æ¬Šé‡ 40%
                keyword_score * 0.25 +      # é—œéµè©æ¬Šé‡ 25%
                pattern_score * 0.2 +       # æ¨¡å¼æ¬Šé‡ 20%
                similarity_score * 0.15     # ç›¸ä¼¼åº¦æ¬Šé‡ 15%
            )
            
            # æ‡‰ç”¨æ’é™¤è¦å‰‡
            final_score = self.apply_exclusion_rules(text_variants, category, combined_score)
            
            category_scores[category] = {
                'total_score': final_score,
                'reserved_score': reserved_score,
                'keyword_score': keyword_score,
                'pattern_score': pattern_score,
                'similarity_score': similarity_score
            }
        
        # æ‰¾å‡ºæœ€ä½³é æ¸¬
        best_category = max(category_scores.keys(), key=lambda x: category_scores[x]['total_score'])
        best_score = category_scores[best_category]['total_score']
        
        # æ’åºæ‰€æœ‰åˆ†æ•¸
        sorted_scores = sorted(category_scores.items(), key=lambda x: x[1]['total_score'], reverse=True)
        
        return {
            'input_text': input_text,
            'processed_variants': text_variants,
            'best_prediction': best_category,
            'best_score': best_score,
            'all_scores': category_scores,
            'sorted_scores': sorted_scores,
            'confidence': best_score
        }

def test_reserved_word_functionality():
    """æ¸¬è©¦ä¿ç•™è©åŠŸèƒ½"""
    print("="*80)
    print("ğŸ§ª æ¸¬è©¦ä¿ç•™è©åŠŸèƒ½")
    print("="*80)
    
    processor = ReservedWordProcessor()
    
    test_cases = [
        "é˜²ç«ç‰†è¨­å‚™",
        "è³‡æ–™åº«ç®¡ç†ç³»çµ±", 
        "å¯æ”œå¼å„²å­˜åª’é«”",
        "å…§éƒ¨äººå“¡",
        "ä½œæ¥­æ–‡ä»¶",
        "ç¶²è·¯æœå‹™",
        "MySQL è³‡æ–™åº«",
        "Windows ä½œæ¥­ç³»çµ±"
    ]
    
    for test_text in test_cases:
        result = processor.process_with_reserved_words(test_text)
        print(f"ğŸ“ æ¸¬è©¦: '{test_text}'")
        print(f"   ä¿ç•™è©: {result['reserved_words']}")
        print(f"   ä¸€èˆ¬è©: {result['regular_words']}")
        print(f"   æ‰€æœ‰è©å…ƒ: {result['all_tokens']}")
        print()

def test_enhanced_classifier_v3():
    """æ¸¬è©¦å¢å¼·ç‰ˆåˆ†é¡å™¨ v3.0"""
    print("="*80)
    print("ğŸ§ª æ¸¬è©¦å¢å¼·ç‰ˆåˆ†é¡å™¨ v3.0 (å«ä¿ç•™è©)")
    print("="*80)
    
    classifier = EnhancedClassifierV3()
    
    # æ¸¬è©¦æ¡ˆä¾‹ï¼ˆé‡é»æ¸¬è©¦ä¿ç•™è©è™•ç†ï¼‰
    test_cases = [
        ("é˜²ç«ç‰†è¨­å‚™", "ç¡¬é«”"),
        ("è³‡æ–™åº«ç®¡ç†ç³»çµ±", "è»Ÿé«”"),
        ("å¯æ”œå¼å„²å­˜åª’é«”", "å¯¦é«”"),
        ("å…§éƒ¨äººå“¡", "äººå“¡"),
        ("å¤–éƒ¨äººå“¡", "äººå“¡"),
        ("ä½œæ¥­æ–‡ä»¶", "è³‡æ–™"),
        ("é›»å­ç´€éŒ„", "è³‡æ–™"),
        ("ç¶²è·¯æœå‹™", "æœå‹™"),
        ("é›²ç«¯æœå‹™", "æœå‹™"),
        ("MySQL è³‡æ–™åº«", "è»Ÿé«”"),
        ("Windows ä½œæ¥­ç³»çµ±", "è»Ÿé«”"),
        ("Oracle è³‡æ–™åº«", "è»Ÿé«”"),
        ("æ©Ÿæˆ¿è¨­æ–½", "å¯¦é«”"),
        ("åˆç´„æ–‡ä»¶", "è³‡æ–™"),
        ("æ‰¿è¾¦äºº", "äººå“¡"),
        ("ç®¡ç†å“¡", "äººå“¡"),
        ("å‚™ä»½æª”æ¡ˆ", "è³‡æ–™"),
        ("ç›£æ§è¨­å‚™", "ç¡¬é«”"),
        ("API æœå‹™", "æœå‹™"),
        ("ç¨‹åºæ–‡ä»¶", "è³‡æ–™")
    ]
    
    correct_count = 0
    total_count = len(test_cases)
    
    for i, (test_text, expected) in enumerate(test_cases, 1):
        result = classifier.classify_text(test_text)
        predicted = result['best_prediction']
        is_correct = predicted == expected
        
        if is_correct:
            correct_count += 1
            status = "âœ…"
        else:
            status = "âŒ"
        
        print(f"{status} æ¸¬è©¦ {i}: '{test_text}' â†’ é æ¸¬: {predicted}, å¯¦éš›: {expected}")
        print(f"   ä¿ç•™è©: {result['processed_variants']['reserved_words']}")
        print(f"   ä¿¡å¿ƒåº¦: {result['best_score']:.4f}")
        
        # é¡¯ç¤ºå„åˆ†æ•¸çµ„æˆ
        scores = result['all_scores'][predicted]
        print(f"   åˆ†æ•¸çµ„æˆ - ä¿ç•™è©: {scores['reserved_score']:.3f}, "
              f"é—œéµè©: {scores['keyword_score']:.3f}, "
              f"æ¨¡å¼: {scores['pattern_score']:.3f}, "
              f"ç›¸ä¼¼åº¦: {scores['similarity_score']:.3f}")
        print()
    
    accuracy = correct_count / total_count
    print(f"ğŸ“Š æ¸¬è©¦çµæœ: {correct_count}/{total_count} = {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    return accuracy

if __name__ == "__main__":
    # å…ˆæ¸¬è©¦ä¿ç•™è©åŠŸèƒ½
    test_reserved_word_functionality()
    print("\n" + "="*80 + "\n")
    
    # å†æ¸¬è©¦å®Œæ•´åˆ†é¡å™¨
    test_enhanced_classifier_v3()