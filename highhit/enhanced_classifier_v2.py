#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆè³‡ç”¢åˆ†é¡å™¨ v2.0
é‡å°è®ŠåŒ–ç‰ˆæœ¬è­˜åˆ¥å’Œé¡åˆ¥æ··æ·†å•é¡Œé€²è¡Œå„ªåŒ–
"""

import pandas as pd
import numpy as np
import re
from collections import defaultdict, Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
import jieba.posseg as pseg

class EnhancedClassifierV2:
    """å¢å¼·ç‰ˆåˆ†é¡å™¨ v2.0"""
    
    def __init__(self, data_path='RA_data.csv'):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆåˆ†é¡å™¨
        Args:
            data_path: è³‡æ–™æª”æ¡ˆè·¯å¾‘
        """
        self.data_path = data_path
        self.data = None
        self.category_keywords = {}
        self.category_patterns = {}
        self.exclusion_rules = {}
        self.vectorizer = None
        self.category_vectors = {}
        
        # è¼‰å…¥æ•¸æ“šä¸¦åˆå§‹åŒ–
        self.load_data()
        self.build_enhanced_features()
        self.create_category_rules()
    
    def load_data(self):
        """è¼‰å…¥è³‡æ–™"""
        try:
            self.data = pd.read_csv(self.data_path, encoding='utf-8')
            print(f"âœ… è¼‰å…¥è³‡æ–™ï¼š{len(self.data)} ç­†è¨˜éŒ„")
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—ï¼š{e}")
            self.data = pd.DataFrame()
    
    def preprocess_text(self, text):
        """
        å¢å¼·ç‰ˆæ–‡æœ¬é è™•ç†
        Args:
            text: è¼¸å…¥æ–‡æœ¬
        Returns:
            dict: åŒ…å«å¤šç¨®è™•ç†çµæœçš„å­—å…¸
        """
        if not isinstance(text, str):
            text = str(text)
        
        # åŸå§‹æ–‡æœ¬
        original = text.strip()
        
        # åŸºæœ¬æ¸…ç†
        cleaned = re.sub(r'\s+', ' ', text.strip())
        
        # ç§»é™¤æ‹¬è™Ÿå…§å®¹
        no_brackets = re.sub(r'\([^)]*\)', '', cleaned).strip()
        
        # æå–æ‹¬è™Ÿå…§å®¹
        bracket_content = ""
        bracket_match = re.search(r'\(([^)]*)\)', cleaned)
        if bracket_match:
            bracket_content = bracket_match.group(1).strip()
        
        # å¤§å°å¯«è®ŠåŒ–
        lower_case = cleaned.lower()
        
        # ç§»é™¤ç©ºæ ¼
        no_spaces = re.sub(r'\s+', '', cleaned)
        
        # åˆ†è©
        words = list(jieba.cut(cleaned))
        words_no_stop = [w for w in words if len(w.strip()) > 1 and w.strip() not in ['çš„', 'èˆ‡', 'åŠ', 'å’Œ', 'æˆ–']]
        
        # è©æ€§æ¨™è¨»
        pos_tags = [(word, flag) for word, flag in pseg.cut(cleaned)]
        
        return {
            'original': original,
            'cleaned': cleaned,
            'no_brackets': no_brackets,
            'bracket_content': bracket_content,
            'lower_case': lower_case,
            'no_spaces': no_spaces,
            'words': words,
            'words_no_stop': words_no_stop,
            'pos_tags': pos_tags
        }
    
    def build_enhanced_features(self):
        """å»ºç«‹å¢å¼·ç‰¹å¾µ"""
        if self.data.empty:
            return
        
        # ç‚ºæ¯å€‹é¡åˆ¥å»ºç«‹é—œéµè©é›†åˆ
        category_texts = defaultdict(list)
        
        for _, row in self.data.iterrows():
            category = row['è³‡ç”¢é¡åˆ¥']
            asset_name = row['è³‡ç”¢åç¨±']
            
            # é è™•ç†è³‡ç”¢åç¨±
            processed = self.preprocess_text(asset_name)
            
            # æ”¶é›†è©²é¡åˆ¥çš„æ‰€æœ‰æ–‡æœ¬è®ŠåŒ–
            category_texts[category].extend([
                processed['cleaned'],
                processed['no_brackets'],
                processed['bracket_content'],
                ' '.join(processed['words_no_stop'])
            ])
        
        # å»ºç«‹ TF-IDF å‘é‡åŒ–å™¨
        all_texts = []
        category_labels = []
        
        for category, texts in category_texts.items():
            for text in texts:
                if text.strip():
                    all_texts.append(text)
                    category_labels.append(category)
        
        # ä½¿ç”¨å­—ç¬¦ç´šåˆ¥çš„ n-gram æé«˜å°è®ŠåŒ–ç‰ˆæœ¬çš„è­˜åˆ¥èƒ½åŠ›
        self.vectorizer = TfidfVectorizer(
            analyzer='char_wb',
            ngram_range=(2, 4),
            max_features=5000,
            lowercase=True
        )
        
        if all_texts:
            vectors = self.vectorizer.fit_transform(all_texts)
            
            # ç‚ºæ¯å€‹é¡åˆ¥è¨ˆç®—å¹³å‡å‘é‡
            for category in category_texts.keys():
                category_indices = [i for i, label in enumerate(category_labels) if label == category]
                if category_indices:
                    category_vector = vectors[category_indices].mean(axis=0)
                    self.category_vectors[category] = category_vector
    
    def create_category_rules(self):
        """å‰µå»ºé¡åˆ¥è¦å‰‡"""
        # å¼·åŒ–çš„é—œéµè©è¦å‰‡
        self.category_keywords = {
            'è»Ÿé«”': {
                'strong': ['ç³»çµ±', 'è»Ÿé«”', 'æ‡‰ç”¨ç¨‹å¼', 'è³‡æ–™åº«', 'ç¨‹å¼', 'èªè¨€', 'å¹³å°', 'æ¡†æ¶'],
                'medium': ['server', 'sql', 'windows', 'linux', 'unix', 'java', 'python', '.net', 'asp'],
                'weak': ['ç®¡ç†', 'é–‹ç™¼', 'æœå‹™å™¨']
            },
            'ç¡¬é«”': {
                'strong': ['ç¡¬é«”', 'è¨­å‚™', 'ä¼ºæœå™¨', 'ä¸»æ©Ÿ', 'é›»è…¦', 'ç¶²è·¯è¨­å‚™', 'å„²å­˜'],
                'medium': ['server', 'äº¤æ›å™¨', 'è·¯ç”±å™¨', 'é˜²ç«ç‰†', 'å°è¡¨æ©Ÿ'],
                'weak': ['æ©Ÿå™¨', 'è¨­æ–½', 'çµ‚ç«¯']
            },
            'å¯¦é«”': {
                'strong': ['å¯¦é«”', 'ç’°å¢ƒ', 'è¨­æ–½', 'å ´æ‰€', 'ç©ºé–“', 'æ©Ÿæˆ¿', 'è¾¦å…¬å®¤'],
                'medium': ['å»ºç¯‰', 'å ´åœ°', 'ä½ç½®', 'å€åŸŸ'],
                'weak': ['åœ°é»', 'è™•æ‰€']
            },
            'è³‡æ–™': {
                'strong': ['è³‡æ–™', 'æ–‡ä»¶', 'æª”æ¡ˆ', 'ç´€éŒ„', 'åˆç´„', 'æ–‡æª”'],
                'medium': ['ä½œæ¥­', 'ç¨‹åº', 'sop', 'å‚™ä»½', 'æ—¥èªŒ', 'åŸå§‹ç¢¼'],
                'weak': ['ç´€éŒ„', 'è³‡è¨Š', 'å…§å®¹']
            },
            'äººå“¡': {
                'strong': ['äººå“¡', 'å“¡å·¥', 'è·å“¡', 'ä½¿ç”¨è€…', 'ç”¨æˆ¶', 'ç®¡ç†å“¡'],
                'medium': ['å…§éƒ¨', 'å¤–éƒ¨', 'å®¢æˆ¶', 'å» å•†'],
                'weak': ['äºº', 'è€…']
            },
            'æœå‹™': {
                'strong': ['æœå‹™', 'æ‡‰ç”¨', 'ç³»çµ±æœå‹™', 'ç¶²è·¯æœå‹™', 'é›²ç«¯æœå‹™'],
                'medium': ['api', 'web', 'ç¶²ç«™', 'å…¥å£ç¶²ç«™'],
                'weak': ['åŠŸèƒ½', 'æ”¯æ´']
            }
        }
        
        # æ’é™¤è¦å‰‡ - é¿å…éŒ¯èª¤åˆ†é¡
        self.exclusion_rules = {
            'äººå“¡': {
                # åŒ…å«é€™äº›è©çš„ä¸æ‡‰è©²åˆ†é¡ç‚ºäººå“¡
                'exclude_if_contains': ['æ–‡ä»¶', 'æª”æ¡ˆ', 'è³‡æ–™', 'ç¨‹åº', 'ç³»çµ±', 'è¨­å‚™', 'æœå‹™']
            },
            'è³‡æ–™': {
                # åŒ…å«é€™äº›è©çš„æ›´å¯èƒ½æ˜¯è³‡æ–™é¡
                'include_if_contains': ['æ–‡ä»¶', 'æª”æ¡ˆ', 'ç´€éŒ„', 'åˆç´„', 'ä½œæ¥­', 'sop']
            }
        }
        
        # æ­£å‰‡è¡¨é”å¼æ¨¡å¼
        self.category_patterns = {
            'è»Ÿé«”': [
                r'.*ç³»çµ±$', r'.*è»Ÿé«”$', r'.*ç¨‹å¼.*', r'.*è³‡æ–™åº«.*',
                r'.*(windows|linux|unix|sql|java|python|\.net).*'
            ],
            'ç¡¬é«”': [
                r'.*è¨­å‚™$', r'.*ä¸»æ©Ÿ$', r'.*ä¼ºæœå™¨.*', r'.*é›»è…¦.*',
                r'.*(server|äº¤æ›å™¨|è·¯ç”±å™¨|é˜²ç«ç‰†).*'
            ],
            'è³‡æ–™': [
                r'.*æ–‡ä»¶.*', r'.*æª”æ¡ˆ.*', r'.*ç´€éŒ„.*', r'.*åˆç´„.*',
                r'.*(sop|å‚™ä»½|æ—¥èªŒ|åŸå§‹ç¢¼).*'
            ],
            'äººå“¡': [
                r'.*äººå“¡$', r'.*å“¡å·¥.*', r'.*ä½¿ç”¨è€….*', r'.*ç®¡ç†å“¡.*'
            ],
            'æœå‹™': [
                r'.*æœå‹™.*', r'.*æ‡‰ç”¨.*', r'.*(api|web|ç¶²ç«™).*'
            ]
        }
    
    def calculate_keyword_score(self, text_variants, category):
        """
        è¨ˆç®—é—œéµè©åŒ¹é…åˆ†æ•¸
        Args:
            text_variants: æ–‡æœ¬çš„å„ç¨®è®ŠåŒ–å½¢å¼
            category: ç›®æ¨™é¡åˆ¥
        Returns:
            float: é—œéµè©åŒ¹é…åˆ†æ•¸
        """
        if category not in self.category_keywords:
            return 0.0
        
        keywords = self.category_keywords[category]
        score = 0.0
        total_weight = 0.0
        
        # æª¢æŸ¥æ‰€æœ‰æ–‡æœ¬è®ŠåŒ–
        all_text = ' '.join([
            text_variants.get('cleaned', ''),
            text_variants.get('no_brackets', ''),
            text_variants.get('bracket_content', ''),
            ' '.join(text_variants.get('words_no_stop', []))
        ]).lower()
        
        # å¼·é—œéµè© (æ¬Šé‡ 3.0)
        for keyword in keywords.get('strong', []):
            if keyword in all_text:
                score += 3.0
                total_weight += 3.0
        
        # ä¸­é—œéµè© (æ¬Šé‡ 2.0)
        for keyword in keywords.get('medium', []):
            if keyword in all_text:
                score += 2.0
                total_weight += 2.0
        
        # å¼±é—œéµè© (æ¬Šé‡ 1.0)
        for keyword in keywords.get('weak', []):
            if keyword in all_text:
                score += 1.0
                total_weight += 1.0
        
        # æ­£è¦åŒ–åˆ†æ•¸
        max_possible_score = (
            len(keywords.get('strong', [])) * 3.0 +
            len(keywords.get('medium', [])) * 2.0 +
            len(keywords.get('weak', [])) * 1.0
        )
        
        return score / max_possible_score if max_possible_score > 0 else 0.0
    
    def calculate_pattern_score(self, text_variants, category):
        """
        è¨ˆç®—æ¨¡å¼åŒ¹é…åˆ†æ•¸
        Args:
            text_variants: æ–‡æœ¬çš„å„ç¨®è®ŠåŒ–å½¢å¼
            category: ç›®æ¨™é¡åˆ¥
        Returns:
            float: æ¨¡å¼åŒ¹é…åˆ†æ•¸
        """
        if category not in self.category_patterns:
            return 0.0
        
        patterns = self.category_patterns[category]
        
        # æª¢æŸ¥æ‰€æœ‰æ–‡æœ¬è®ŠåŒ–
        test_texts = [
            text_variants.get('cleaned', ''),
            text_variants.get('no_brackets', ''),
            text_variants.get('bracket_content', ''),
            text_variants.get('lower_case', '')
        ]
        
        match_count = 0
        for pattern in patterns:
            for text in test_texts:
                if text and re.search(pattern, text, re.IGNORECASE):
                    match_count += 1
                    break  # ä¸€å€‹æ¨¡å¼åŒ¹é…å°±è¶³å¤ 
        
        return match_count / len(patterns) if patterns else 0.0
    
    def calculate_similarity_score(self, text_variants, category):
        """
        è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸
        Args:
            text_variants: æ–‡æœ¬çš„å„ç¨®è®ŠåŒ–å½¢å¼  
            category: ç›®æ¨™é¡åˆ¥
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•¸
        """
        if not self.vectorizer or category not in self.category_vectors:
            return 0.0
        
        # çµ„åˆæ‰€æœ‰æ–‡æœ¬è®ŠåŒ–
        combined_text = ' '.join([
            text_variants.get('cleaned', ''),
            text_variants.get('no_brackets', ''),
            text_variants.get('bracket_content', '')
        ]).strip()
        
        if not combined_text:
            return 0.0
        
        try:
            # å‘é‡åŒ–è¼¸å…¥æ–‡æœ¬
            input_vector = self.vectorizer.transform([combined_text])
            
            # è¨ˆç®—èˆ‡é¡åˆ¥å‘é‡çš„ç›¸ä¼¼åº¦
            similarity = cosine_similarity(input_vector, self.category_vectors[category])[0, 0]
            return float(similarity)
        except:
            return 0.0
    
    def apply_exclusion_rules(self, text_variants, category, base_score):
        """
        æ‡‰ç”¨æ’é™¤è¦å‰‡
        Args:
            text_variants: æ–‡æœ¬çš„å„ç¨®è®ŠåŒ–å½¢å¼
            category: ç›®æ¨™é¡åˆ¥
            base_score: åŸºç¤åˆ†æ•¸
        Returns:
            float: èª¿æ•´å¾Œçš„åˆ†æ•¸
        """
        if category not in self.exclusion_rules:
            return base_score
        
        rules = self.exclusion_rules[category]
        combined_text = ' '.join([
            text_variants.get('cleaned', ''),
            text_variants.get('no_brackets', ''),
            text_variants.get('bracket_content', '')
        ]).lower()
        
        # æ’é™¤è¦å‰‡
        if 'exclude_if_contains' in rules:
            for exclude_word in rules['exclude_if_contains']:
                if exclude_word in combined_text:
                    return base_score * 0.3  # å¤§å¹…é™ä½åˆ†æ•¸
        
        # åŒ…å«è¦å‰‡ (å¢å¼·ç‰¹å®šé¡åˆ¥)
        if 'include_if_contains' in rules:
            for include_word in rules['include_if_contains']:
                if include_word in combined_text:
                    return base_score * 1.5  # å¢å¼·åˆ†æ•¸
        
        return base_score
    
    def classify_text(self, input_text, method='enhanced'):
        """
        å¢å¼·ç‰ˆæ–‡æœ¬åˆ†é¡
        Args:
            input_text: è¼¸å…¥æ–‡æœ¬
            method: åˆ†é¡æ–¹æ³•
        Returns:
            dict: åˆ†é¡çµæœ
        """
        if self.data.empty:
            return {'error': 'æ²’æœ‰å¯ç”¨çš„è¨“ç·´è³‡æ–™'}
        
        # é è™•ç†è¼¸å…¥æ–‡æœ¬
        text_variants = self.preprocess_text(input_text)
        
        # ç²å–æ‰€æœ‰é¡åˆ¥
        categories = self.data['è³‡ç”¢é¡åˆ¥'].unique()
        category_scores = {}
        
        for category in categories:
            # è¨ˆç®—å¤šç¨®åˆ†æ•¸
            keyword_score = self.calculate_keyword_score(text_variants, category)
            pattern_score = self.calculate_pattern_score(text_variants, category)
            similarity_score = self.calculate_similarity_score(text_variants, category)
            
            # åŠ æ¬Šçµ„åˆåˆ†æ•¸
            combined_score = (
                keyword_score * 0.4 +      # é—œéµè©æ¬Šé‡ 40%
                pattern_score * 0.3 +      # æ¨¡å¼æ¬Šé‡ 30%
                similarity_score * 0.3     # ç›¸ä¼¼åº¦æ¬Šé‡ 30%
            )
            
            # æ‡‰ç”¨æ’é™¤è¦å‰‡
            final_score = self.apply_exclusion_rules(text_variants, category, combined_score)
            
            category_scores[category] = {
                'total_score': final_score,
                'keyword_score': keyword_score,
                'pattern_score': pattern_score,
                'similarity_score': similarity_score
            }
        
        # æ‰¾å‡ºæœ€ä½³é æ¸¬
        best_category = max(category_scores.keys(), key=lambda x: category_scores[x]['total_score'])
        best_score = category_scores[best_category]['total_score']
        
        # æ’åºæ‰€æœ‰åˆ†æ•¸
        sorted_scores = sorted(category_scores.items(), key=lambda x: x[1]['total_score'], reverse=True)
        
        return {
            'input_text': input_text,
            'processed_variants': text_variants,
            'best_prediction': best_category,
            'best_score': best_score,
            'all_scores': category_scores,
            'sorted_scores': sorted_scores,
            'confidence': best_score
        }

def test_enhanced_classifier():
    """æ¸¬è©¦å¢å¼·ç‰ˆåˆ†é¡å™¨"""
    print("="*80)
    print("ğŸ§ª æ¸¬è©¦å¢å¼·ç‰ˆåˆ†é¡å™¨")
    print("="*80)
    
    classifier = EnhancedClassifierV2()
    
    # æ¸¬è©¦æ¡ˆä¾‹ï¼ˆåŒ…å«ä¹‹å‰çš„éŒ¯èª¤æ¡ˆä¾‹ï¼‰
    test_cases = [
        ("ä½œæ¥­æ–‡ä»¶", "è³‡æ–™"),
        ("é›»å­ç´€éŒ„", "è³‡æ–™"), 
        ("å¯æ”œå¼å„²å­˜åª’é«”", "å¯¦é«”"),
        ("è³‡æ–™åº«ç®¡ç†ç³»çµ±", "è»Ÿé«”"),
        ("é–‹ç™¼èªè¨€", "è»Ÿé«”"),
        ("å¤–éƒ¨äººå“¡", "äººå“¡"),
        ("å…§ã€å¤–éƒ¨æœå‹™", "æœå‹™"),
        ("åˆç´„", "è³‡æ–™"),
        ("MySQL è³‡æ–™åº«", "è»Ÿé«”"),
        ("Windows ä½œæ¥­ç³»çµ±", "è»Ÿé«”"),
        ("é˜²ç«ç‰†è¨­å‚™", "ç¡¬é«”"),
        ("å‚™ä»½æª”æ¡ˆ", "è³‡æ–™")
    ]
    
    correct_count = 0
    total_count = len(test_cases)
    
    for i, (test_text, expected) in enumerate(test_cases, 1):
        result = classifier.classify_text(test_text)
        predicted = result['best_prediction']
        is_correct = predicted == expected
        
        if is_correct:
            correct_count += 1
            status = "âœ…"
        else:
            status = "âŒ"
        
        print(f"{status} æ¸¬è©¦ {i}: '{test_text}' â†’ é æ¸¬: {predicted}, å¯¦éš›: {expected}")
        print(f"   ä¿¡å¿ƒåº¦: {result['best_score']:.4f}")
        
        # é¡¯ç¤ºå‰3å€‹åˆ†æ•¸
        top_3 = result['sorted_scores'][:3]
        for j, (cat, scores) in enumerate(top_3):
            print(f"   {j+1}. {cat}: {scores['total_score']:.4f}")
        print()
    
    accuracy = correct_count / total_count
    print(f"ğŸ“Š æ¸¬è©¦çµæœ: {correct_count}/{total_count} = {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    return accuracy

if __name__ == "__main__":
    test_enhanced_classifier()