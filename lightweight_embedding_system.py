#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¼•é‡ç´šå¢å¼·ç‰ˆ Embedding ç³»çµ±
ä½¿ç”¨æ”¹é€²çš„ TF-IDF æ–¹æ³•ï¼Œä¸ä¾è³´å¤–éƒ¨æ·±åº¦å­¸ç¿’æ¨¡å‹
"""

import pandas as pd
import jieba
import numpy as np
import pickle
import os
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import warnings
warnings.filterwarnings('ignore')

class LightweightEmbeddingSystem:
    def __init__(self, csv_path='RA_data.csv'):
        """
        åˆå§‹åŒ–è¼•é‡ç´š Embedding ç³»çµ±
        Args:
            csv_path: CSV è³‡æ–™æª”æ¡ˆè·¯å¾‘
        """
        self.csv_path = csv_path
        self.embeddings_file = 'ra_embeddings_light.pkl'
        self.metadata_file = 'ra_metadata_light.pkl'
        
        # ä½¿ç”¨å¤šç¨® TF-IDF é…ç½®
        self.vectorizers = {
            'standard': TfidfVectorizer(
                max_features=1500, 
                ngram_range=(1, 2),
                min_df=1,
                max_df=0.95,
                analyzer='word'
            ),
            'char_level': TfidfVectorizer(
                max_features=1000, 
                ngram_range=(2, 4),
                min_df=1,
                max_df=0.95,
                analyzer='char'
            ),
            'extended': TfidfVectorizer(
                max_features=2000, 
                ngram_range=(1, 3),
                min_df=1,
                max_df=0.95,
                analyzer='word'
            )
        }
        
        # è³‡æ–™å­˜å„²
        self.df = None
        self.processed_texts = []
        self.categories = []
        self.asset_names = []
        self.tfidf_matrices = {}
        
    def preprocess_text(self, text):
        """
        å¢å¼·ç‰ˆæ–‡æœ¬é è™•ç†
        Args:
            text: è¼¸å…¥æ–‡æœ¬
        Returns:
            è™•ç†å¾Œçš„æ–‡æœ¬
        """
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’Œæ•¸å­—
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\-\(\)]', ' ', text)
        
        # æ¨™æº–åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ä½¿ç”¨ jieba åˆ†è©
        words = list(jieba.cut(text))
        
        # ä¿ç•™æœ‰æ„ç¾©çš„è©å½™
        meaningful_words = []
        for word in words:
            word = word.strip()
            # ä¿ç•™é•·åº¦å¤§æ–¼1çš„è©ï¼Œæˆ–è€…æ˜¯è‹±æ–‡/æ•¸å­—
            if len(word) > 1 or re.match(r'[a-zA-Z0-9]', word):
                meaningful_words.append(word)
        
        processed = ' '.join(meaningful_words)
        return processed if processed else text
    
    def load_and_process_data(self):
        """è¼‰å…¥ä¸¦é è™•ç†è³‡æ–™"""
        print(f"æ­£åœ¨è¼‰å…¥è³‡æ–™: {self.csv_path}")
        
        try:
            self.df = pd.read_csv(self.csv_path, encoding='utf-8')
            print(f"âœ… è¼‰å…¥äº† {len(self.df)} ç­†è³‡æ–™")
        except Exception as e:
            print(f"âŒ è¼‰å…¥è³‡æ–™å¤±æ•—: {e}")
            return False
        
        # é è™•ç†æ‰€æœ‰è³‡ç”¢åç¨±
        print("æ­£åœ¨é è™•ç†æ–‡æœ¬...")
        for _, row in self.df.iterrows():
            category = row['è³‡ç”¢é¡åˆ¥']
            asset_name = row['è³‡ç”¢åç¨±']
            
            # é è™•ç†è³‡ç”¢åç¨±
            processed_name = self.preprocess_text(asset_name)
            
            self.processed_texts.append(processed_name)
            self.categories.append(category)
            self.asset_names.append(asset_name)
        
        print(f"âœ… é è™•ç†å®Œæˆï¼Œå…± {len(self.processed_texts)} é …è³‡ç”¢")
        return True
    
    def compute_embeddings(self):
        """è¨ˆç®—æ‰€æœ‰è³‡ç”¢çš„å¤šç¨® TF-IDF å‘é‡"""
        if not self.processed_texts:
            print("âŒ æ²’æœ‰å¯è™•ç†çš„æ–‡æœ¬è³‡æ–™")
            return False
        
        print("æ­£åœ¨è¨ˆç®—å¤šç¨® TF-IDF å‘é‡...")
        
        for name, vectorizer in self.vectorizers.items():
            try:
                print(f"ğŸ“Š è¨ˆç®— {name} TF-IDF å‘é‡...")
                matrix = vectorizer.fit_transform(self.processed_texts)
                self.tfidf_matrices[name] = matrix
                print(f"âœ… {name} TF-IDF å®Œæˆï¼Œç¶­åº¦: {matrix.shape}")
            except Exception as e:
                print(f"âŒ {name} TF-IDF è¨ˆç®—å¤±æ•—: {e}")
                return False
        
        return True
    
    def save_embeddings(self):
        """å„²å­˜ embeddings åˆ°æª”æ¡ˆ"""
        print("æ­£åœ¨å„²å­˜ embeddings...")
        
        # æº–å‚™è¦å„²å­˜çš„è³‡æ–™
        embedding_data = {
            'tfidf_matrices': self.tfidf_matrices,
            'vectorizers': self.vectorizers,
            'processed_texts': self.processed_texts,
            'categories': self.categories,
            'asset_names': self.asset_names,
            'created_time': datetime.now().isoformat(),
            'data_hash': hash(str(self.processed_texts))
        }
        
        try:
            with open(self.embeddings_file, 'wb') as f:
                pickle.dump(embedding_data, f)
            print(f"âœ… Embeddings å·²å„²å­˜åˆ°: {self.embeddings_file}")
            
            # å„²å­˜å¾Œè¨­è³‡æ–™
            metadata = {
                'total_assets': len(self.asset_names),
                'categories': list(set(self.categories)),
                'vectorizer_types': list(self.tfidf_matrices.keys()),
                'created_time': datetime.now().isoformat()
            }
            
            with open(self.metadata_file, 'wb') as f:
                pickle.dump(metadata, f)
            print(f"âœ… å¾Œè¨­è³‡æ–™å·²å„²å­˜åˆ°: {self.metadata_file}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å„²å­˜å¤±æ•—: {e}")
            return False
    
    def load_embeddings(self):
        """è¼‰å…¥é å…ˆè¨ˆç®—çš„ embeddings"""
        if not os.path.exists(self.embeddings_file):
            print(f"âš ï¸  æ‰¾ä¸åˆ° embeddings æª”æ¡ˆ: {self.embeddings_file}")
            return False
        
        try:
            print("æ­£åœ¨è¼‰å…¥é å…ˆè¨ˆç®—çš„ embeddings...")
            with open(self.embeddings_file, 'rb') as f:
                data = pickle.load(f)
            
            self.tfidf_matrices = data.get('tfidf_matrices', {})
            self.vectorizers = data.get('vectorizers', {})
            self.processed_texts = data.get('processed_texts', [])
            self.categories = data.get('categories', [])
            self.asset_names = data.get('asset_names', [])
            
            # æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§
            if not self.processed_texts or not self.categories:
                print("âŒ è¼‰å…¥çš„è³‡æ–™ä¸å®Œæ•´")
                return False
            
            print(f"âœ… æˆåŠŸè¼‰å…¥ {len(self.processed_texts)} é …è³‡ç”¢çš„ embeddings")
            print(f"ğŸ“… å»ºç«‹æ™‚é–“: {data.get('created_time', 'Unknown')}")
            print(f"ğŸ”§ å‘é‡åŒ–å™¨é¡å‹: {list(self.tfidf_matrices.keys())}")
            
            return True
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥ embeddings å¤±æ•—: {e}")
            return False
    
    def compute_similarity(self, input_text, top_k=50):
        """
        è¨ˆç®—è¼¸å…¥æ–‡æœ¬èˆ‡æ‰€æœ‰è³‡ç”¢çš„ç›¸ä¼¼åº¦
        Args:
            input_text: è¼¸å…¥æ–‡æœ¬
            top_k: è¿”å›å‰ k å€‹æœ€ç›¸ä¼¼çš„çµæœ
        Returns:
            ç›¸ä¼¼åº¦çµæœåˆ—è¡¨
        """
        if not self.processed_texts:
            print("âŒ æ²’æœ‰è¼‰å…¥çš„ embeddings è³‡æ–™")
            return [], ""
        
        # é è™•ç†è¼¸å…¥æ–‡æœ¬
        processed_input = self.preprocess_text(input_text)
        print(f"è¼¸å…¥æ–‡æœ¬: {input_text}")
        print(f"è™•ç†å¾Œæ–‡æœ¬: {processed_input}")
        
        # ä½¿ç”¨å¤šç¨®å‘é‡åŒ–å™¨è¨ˆç®—ç›¸ä¼¼åº¦
        all_similarities = {}
        weights = {'standard': 0.4, 'char_level': 0.3, 'extended': 0.3}
        
        for name, matrix in self.tfidf_matrices.items():
            try:
                print(f"ğŸ“Š ä½¿ç”¨ {name} è¨ˆç®—ç›¸ä¼¼åº¦...")
                vectorizer = self.vectorizers[name]
                input_vector = vectorizer.transform([processed_input])
                similarities = cosine_similarity(input_vector, matrix)[0]
                all_similarities[name] = similarities
                print(f"âœ… {name} ç›¸ä¼¼åº¦è¨ˆç®—å®Œæˆ")
            except Exception as e:
                print(f"âŒ {name} ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
                continue
        
        if not all_similarities:
            print("âŒ æ‰€æœ‰ç›¸ä¼¼åº¦è¨ˆç®—æ–¹æ³•éƒ½å¤±æ•—")
            return [], processed_input
        
        # åŠ æ¬Šåˆä½µç›¸ä¼¼åº¦
        print("ğŸ”„ åˆä½µå¤šç¨®ç›¸ä¼¼åº¦åˆ†æ•¸...")
        final_similarities = np.zeros(len(self.processed_texts))
        
        for name, similarities in all_similarities.items():
            weight = weights.get(name, 1.0 / len(all_similarities))
            final_similarities += weight * similarities
        
        # å»ºç«‹çµæœåˆ—è¡¨
        results = []
        for i, sim in enumerate(final_similarities):
            if i < len(self.categories) and i < len(self.asset_names):
                results.append({
                    'similarity': float(sim),
                    'category': self.categories[i],
                    'asset_name': self.asset_names[i],
                    'processed_text': self.processed_texts[i],
                    'index': i
                })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºä¸¦å–å‰ k å€‹
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k], processed_input
    
    def build_embeddings(self, force_rebuild=False):
        """
        å»ºç«‹ embeddingsï¼ˆä¸»è¦å…¥å£é»ï¼‰
        Args:
            force_rebuild: æ˜¯å¦å¼·åˆ¶é‡å»º
        """
        print("="*80)
        print("ğŸš€ è¼•é‡ç´šå¢å¼·ç‰ˆ Embedding ç³»çµ±")
        print("="*80)
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        if not force_rebuild and self.load_embeddings():
            print("âœ… ä½¿ç”¨ç¾æœ‰çš„ embeddings")
            return True
        
        print("ğŸ”„ éœ€è¦é‡å»º embeddings...")
        
        # è¼‰å…¥å’Œé è™•ç†è³‡æ–™
        if not self.load_and_process_data():
            return False
        
        # è¨ˆç®— embeddings
        if not self.compute_embeddings():
            return False
        
        # å„²å­˜ embeddings
        if not self.save_embeddings():
            return False
        
        print("ğŸ‰ Embeddings å»ºç«‹å®Œæˆï¼")
        return True
    
    def get_statistics(self):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        if not self.categories:
            return {}
        
        category_count = {}
        for cat in self.categories:
            category_count[cat] = category_count.get(cat, 0) + 1
        
        return {
            'total_assets': len(self.asset_names),
            'categories': category_count,
            'embedding_method': "å¤šé‡ TF-IDF",
            'vectorizer_types': list(self.tfidf_matrices.keys()),
            'matrix_shapes': {name: matrix.shape for name, matrix in self.tfidf_matrices.items()}
        }

def main():
    """ä¸»ç¨‹å¼ï¼šå»ºç«‹å’Œæ¸¬è©¦ embeddings"""
    # å»ºç«‹ embedding ç³»çµ±
    embedding_system = LightweightEmbeddingSystem()
    
    # å»ºç«‹ embeddings
    if not embedding_system.build_embeddings():
        print("âŒ Embeddings å»ºç«‹å¤±æ•—")
        return
    
    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    stats = embedding_system.get_statistics()
    print("\nğŸ“Š ç³»çµ±çµ±è¨ˆè³‡è¨Š:")
    print(f"ç¸½è³‡ç”¢æ•¸é‡: {stats.get('total_assets', 0)}")
    print(f"Embedding æ–¹æ³•: {stats.get('embedding_method', 'Unknown')}")
    print(f"å‘é‡åŒ–å™¨é¡å‹: {stats.get('vectorizer_types', [])}")
    
    print("\nå‘é‡çŸ©é™£ç¶­åº¦:")
    for vtype, shape in stats.get('matrix_shapes', {}).items():
        print(f"  {vtype}: {shape}")
    
    print("\nå„é¡åˆ¥è³‡ç”¢çµ±è¨ˆ:")
    for category, count in stats.get('categories', {}).items():
        print(f"  {category}: {count} é …")
    
    # æ¸¬è©¦ç›¸ä¼¼åº¦è¨ˆç®—
    print("\n" + "="*80)
    print("ğŸ§ª æ¸¬è©¦ç›¸ä¼¼åº¦è¨ˆç®—")
    print("="*80)
    
    test_cases = [
        "MySQL è³‡æ–™åº«ç®¡ç†ç³»çµ±",
        "å‚™ä»½æª”æ¡ˆå’Œæ—¥èªŒè¨˜éŒ„",
        "ç³»çµ±ç®¡ç†å“¡æ¬Šé™",
        "Windows ä½œæ¥­ç³»çµ±"
    ]
    
    for test_text in test_cases:
        print(f"\nğŸ” æ¸¬è©¦: {test_text}")
        results, processed = embedding_system.compute_similarity(test_text, top_k=5)
        
        if results:
            print(f"å‰ 5 å€‹æœ€ç›¸ä¼¼çš„é …ç›®:")
            for i, result in enumerate(results, 1):
                print(f"{i}. ã€{result['category']}ã€‘{result['asset_name']} "
                      f"(ç›¸ä¼¼åº¦: {result['similarity']:.4f})")
        else:
            print("âŒ æ²’æ‰¾åˆ°ç›¸ä¼¼é …ç›®")

if __name__ == "__main__":
    main()