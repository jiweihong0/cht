#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆç›¸ä¼¼åº¦åˆ†æå·¥å…·
ä½¿ç”¨é å…ˆè¨ˆç®—çš„ embeddings ä¾†æå‡ç›¸ä¼¼åº¦æ¯”å°çš„æº–ç¢ºæ€§å’Œé€Ÿåº¦
"""

import pandas as pd
import numpy as np
from enhanced_embedding_system import EnhancedEmbeddingSystem
import warnings
warnings.filterwarnings('ignore')

class EnhancedSimilarityAnalyzer:
    def __init__(self, csv_path='RA_data.csv'):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆç›¸ä¼¼åº¦åˆ†æå™¨
        Args:
            csv_path: CSV è³‡æ–™æª”æ¡ˆè·¯å¾‘
        """
        self.csv_path = csv_path
        self.embedding_system = EnhancedEmbeddingSystem(csv_path)
        self.is_initialized = False
    
    def initialize(self, force_rebuild=False):
        """
        åˆå§‹åŒ– embedding ç³»çµ±
        Args:
            force_rebuild: æ˜¯å¦å¼·åˆ¶é‡å»º embeddings
        """
        print("æ­£åœ¨åˆå§‹åŒ–å¢å¼·ç‰ˆç›¸ä¼¼åº¦åˆ†æç³»çµ±...")
        
        # å»ºç«‹æˆ–è¼‰å…¥ embeddings
        if self.embedding_system.build_embeddings(force_rebuild=force_rebuild):
            self.is_initialized = True
            print("âœ… ç³»çµ±åˆå§‹åŒ–æˆåŠŸ")
            return True
        else:
            print("âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—")
            return False
    
    def analyze_similarity(self, test_text, top_k=50):
        """
        åˆ†ææ¸¬è©¦æ–‡æœ¬èˆ‡æ‰€æœ‰è¨“ç·´è³‡æ–™çš„ç›¸ä¼¼åº¦
        Args:
            test_text: æ¸¬è©¦æ–‡æœ¬
            top_k: è¿”å›å‰ k å€‹æœ€ç›¸ä¼¼çš„çµæœ
        Returns:
            (results, processed_text) å…ƒçµ„
        """
        if not self.is_initialized:
            print("âš ï¸  ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
            if not self.initialize():
                return [], ""
        
        # ä½¿ç”¨ embedding ç³»çµ±è¨ˆç®—ç›¸ä¼¼åº¦
        results, processed_text = self.embedding_system.compute_similarity(test_text, top_k=top_k)
        
        return results, processed_text
    
    def print_category_analysis(self, results):
        """
        æŒ‰è³‡ç”¢é¡åˆ¥åˆ†çµ„åˆ†æ
        Args:
            results: ç›¸ä¼¼åº¦åˆ†æçµæœ
        """
        if not results:
            print("æ²’æœ‰åˆ†æçµæœå¯é¡¯ç¤º")
            return
        
        print("\n=== æŒ‰è³‡ç”¢é¡åˆ¥åˆ†çµ„çš„ç›¸ä¼¼åº¦åˆ†æ ===")
        
        # æŒ‰é¡åˆ¥åˆ†çµ„
        category_groups = {}
        for result in results:
            category = result['category']
            if category not in category_groups:
                category_groups[category] = []
            category_groups[category].append(result)
        
        # è¨ˆç®—æ¯å€‹é¡åˆ¥çš„çµ±è¨ˆè³‡è¨Š
        category_stats = {}
        for category, items in category_groups.items():
            similarities = [item['similarity'] for item in items]
            category_stats[category] = {
                'avg_similarity': np.mean(similarities),
                'max_similarity': np.max(similarities),
                'min_similarity': np.min(similarities),
                'std_similarity': np.std(similarities),
                'count': len(items),
                'items': items
            }
        
        # æŒ‰å¹³å‡ç›¸ä¼¼åº¦æ’åº
        sorted_categories = sorted(category_stats.items(), 
                                 key=lambda x: x[1]['avg_similarity'], 
                                 reverse=True)
        
        for category, stats in sorted_categories:
            print(f"\nã€{category}ã€‘é¡åˆ¥ (å…±{stats['count']}é …)")
            print(f"  å¹³å‡ç›¸ä¼¼åº¦: {stats['avg_similarity']:.4f}")
            print(f"  æœ€é«˜ç›¸ä¼¼åº¦: {stats['max_similarity']:.4f}")
            print(f"  æœ€ä½ç›¸ä¼¼åº¦: {stats['min_similarity']:.4f}")
            print(f"  æ¨™æº–å·®:     {stats['std_similarity']:.4f}")
            
            # é¡¯ç¤ºè©²é¡åˆ¥ä¸­ç›¸ä¼¼åº¦æœ€é«˜çš„å‰3é …
            top_items = sorted(stats['items'], 
                             key=lambda x: x['similarity'], 
                             reverse=True)[:3]
            
            print("  æœ€ç›¸ä¼¼çš„é …ç›®:")
            for i, item in enumerate(top_items, 1):
                print(f"    {i}. {item['asset_name']} (ç›¸ä¼¼åº¦: {item['similarity']:.4f})")
    
    def print_top_similarities(self, results, top_n=10):
        """
        é¡¯ç¤ºæœ€ç›¸ä¼¼çš„é …ç›®
        Args:
            results: ç›¸ä¼¼åº¦åˆ†æçµæœ
            top_n: é¡¯ç¤ºå‰ n å€‹çµæœ
        """
        if not results:
            print("æ²’æœ‰åˆ†æçµæœå¯é¡¯ç¤º")
            return
        
        print(f"\n=== æœ€ç›¸ä¼¼çš„å‰ {top_n} å€‹é …ç›® ===")
        
        for i, result in enumerate(results[:top_n], 1):
            print(f"{i:2d}. ã€{result['category']}ã€‘{result['asset_name']}")
            print(f"     ç›¸ä¼¼åº¦: {result['similarity']:.4f}")
            if i <= 5:  # åªå°å‰5é …é¡¯ç¤ºè©³ç´°è³‡è¨Š
                print(f"     è™•ç†å¾Œ: {result['processed_text']}")
            print()
    
    def get_best_category_prediction(self, results, method='weighted_avg'):
        """
        åŸºæ–¼ç›¸ä¼¼åº¦çµæœé æ¸¬æœ€ä½³é¡åˆ¥
        Args:
            results: ç›¸ä¼¼åº¦åˆ†æçµæœ
            method: é æ¸¬æ–¹æ³•
                - 'top1': ä½¿ç”¨æœ€ç›¸ä¼¼é …ç›®çš„é¡åˆ¥
                - 'weighted_avg': ä½¿ç”¨åŠ æ¬Šå¹³å‡
                - 'majority_vote': ä½¿ç”¨å‰ké …çš„å¤šæ•¸æŠ•ç¥¨
        Returns:
            é æ¸¬çš„é¡åˆ¥
        """
        if not results:
            return None
        
        if method == 'top1':
            return results[0]['category']
        
        elif method == 'weighted_avg':
            # ä½¿ç”¨å‰10é …è¨ˆç®—åŠ æ¬Šå¹³å‡
            top_results = results[:10]
            category_scores = {}
            
            for result in top_results:
                category = result['category']
                similarity = result['similarity']
                
                if category not in category_scores:
                    category_scores[category] = 0
                category_scores[category] += similarity
            
            # è¿”å›å¾—åˆ†æœ€é«˜çš„é¡åˆ¥
            if category_scores:
                return max(category_scores.items(), key=lambda x: x[1])[0]
        
        elif method == 'majority_vote':
            # ä½¿ç”¨å‰5é …é€²è¡Œå¤šæ•¸æŠ•ç¥¨
            top_results = results[:5]
            category_votes = {}
            
            for result in top_results:
                category = result['category']
                category_votes[category] = category_votes.get(category, 0) + 1
            
            # è¿”å›ç¥¨æ•¸æœ€å¤šçš„é¡åˆ¥
            if category_votes:
                return max(category_votes.items(), key=lambda x: x[1])[0]
        
        return results[0]['category'] if results else None
    
    def analyze_confidence(self, results):
        """
        åˆ†æé æ¸¬çš„ä¿¡å¿ƒåº¦
        Args:
            results: ç›¸ä¼¼åº¦åˆ†æçµæœ
        Returns:
            ä¿¡å¿ƒåº¦è³‡è¨Šå­—å…¸
        """
        if not results:
            return {'confidence': 0, 'reason': 'æ²’æœ‰æ‰¾åˆ°ç›¸ä¼¼é …ç›®'}
        
        top_sim = results[0]['similarity']
        
        # åŸºæ–¼æœ€é«˜ç›¸ä¼¼åº¦åˆ¤æ–·ä¿¡å¿ƒåº¦
        if top_sim >= 0.8:
            confidence_level = 'very_high'
            confidence_desc = "éå¸¸é«˜ä¿¡å¿ƒåº¦"
        elif top_sim >= 0.6:
            confidence_level = 'high'
            confidence_desc = "é«˜ä¿¡å¿ƒåº¦"
        elif top_sim >= 0.4:
            confidence_level = 'medium'
            confidence_desc = "ä¸­ç­‰ä¿¡å¿ƒåº¦"
        elif top_sim >= 0.2:
            confidence_level = 'low'
            confidence_desc = "ä½ä¿¡å¿ƒåº¦"
        else:
            confidence_level = 'very_low'
            confidence_desc = "æ¥µä½ä¿¡å¿ƒåº¦"
        
        # æª¢æŸ¥å‰å¹¾é …æ˜¯å¦ç‚ºåŒä¸€é¡åˆ¥
        top_5_categories = [r['category'] for r in results[:5]]
        same_category_ratio = top_5_categories.count(results[0]['category']) / len(top_5_categories)
        
        return {
            'confidence_level': confidence_level,
            'confidence_desc': confidence_desc,
            'top_similarity': top_sim,
            'same_category_ratio': same_category_ratio,
            'consensus': same_category_ratio >= 0.6,
            'reason': f"æœ€é«˜ç›¸ä¼¼åº¦: {top_sim:.3f}, å‰5é …åŒé¡åˆ¥æ¯”ä¾‹: {same_category_ratio:.1%}"
        }
    
    def get_system_info(self):
        """ç²å–ç³»çµ±è³‡è¨Š"""
        if not self.is_initialized:
            return "ç³»çµ±å°šæœªåˆå§‹åŒ–"
        
        return self.embedding_system.get_statistics()

def analyze_text_similarity_enhanced(test_text, csv_path='RA_data.csv', force_rebuild=False):
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šä½¿ç”¨å¢å¼·ç‰ˆç³»çµ±åˆ†æå–®ä¸€æ–‡æœ¬çš„ç›¸ä¼¼åº¦
    Args:
        test_text: æ¸¬è©¦æ–‡æœ¬
        csv_path: CSV è³‡æ–™æª”æ¡ˆè·¯å¾‘
        force_rebuild: æ˜¯å¦å¼·åˆ¶é‡å»º embeddings
    Returns:
        ç›¸ä¼¼åº¦åˆ†æçµæœ
    """
    analyzer = EnhancedSimilarityAnalyzer(csv_path)
    
    # åˆå§‹åŒ–ç³»çµ±
    if not analyzer.initialize(force_rebuild=force_rebuild):
        print("âŒ ç„¡æ³•åˆå§‹åŒ–å¢å¼·ç‰ˆç›¸ä¼¼åº¦åˆ†æç³»çµ±")
        return []
    
    # é€²è¡Œç›¸ä¼¼åº¦åˆ†æ
    results, processed_text = analyzer.analyze_similarity(test_text)
    
    if not results:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ç›¸ä¼¼é …ç›®")
        return []
    
    # é¡¯ç¤ºåˆ†æçµæœ
    print("="*80)
    print(f"ğŸ” å¢å¼·ç‰ˆç›¸ä¼¼åº¦åˆ†æçµæœ")
    print("="*80)
    print(f"è¼¸å…¥æ–‡æœ¬: {test_text}")
    print(f"è™•ç†å¾Œæ–‡æœ¬: {processed_text}")
    
    # é æ¸¬æœ€ä½³é¡åˆ¥
    predicted_category = analyzer.get_best_category_prediction(results, method='weighted_avg')
    confidence_info = analyzer.analyze_confidence(results)
    
    print(f"\nğŸ¯ é æ¸¬çµæœ:")
    print(f"é æ¸¬é¡åˆ¥: ã€{predicted_category}ã€‘")
    print(f"ä¿¡å¿ƒåº¦: {confidence_info['confidence_desc']} ({confidence_info['top_similarity']:.3f})")
    print(f"é¡åˆ¥å…±è­˜: {'âœ… æ˜¯' if confidence_info['consensus'] else 'âš ï¸ å¦'}")
    print(f"åˆ†æä¾æ“š: {confidence_info['reason']}")
    
    # é¡¯ç¤ºè©³ç´°çµæœ
    analyzer.print_top_similarities(results, top_n=10)
    analyzer.print_category_analysis(results)
    
    return results

if __name__ == "__main__":
    # æ¸¬è©¦ç¯„ä¾‹
    test_cases = [
        "MySQL è³‡æ–™åº«ç®¡ç†ç³»çµ±",
        "å‚™ä»½æª”æ¡ˆå’Œæ—¥èªŒè¨˜éŒ„",
        "ç³»çµ±ç®¡ç†å“¡æ¬Šé™",
        "Windows ä½œæ¥­ç³»çµ±",
        "é˜²ç«ç‰†è¨­å‚™"
    ]
    
    for i, test_text in enumerate(test_cases, 1):
        print(f"\n{'='*100}")
        print(f"ğŸ§ª æ¸¬è©¦æ¡ˆä¾‹ {i}: {test_text}")
        print('='*100)
        
        results = analyze_text_similarity_enhanced(test_text)
        
        if i < len(test_cases):
            input("\næŒ‰ Enter ç¹¼çºŒä¸‹ä¸€å€‹æ¸¬è©¦æ¡ˆä¾‹...")